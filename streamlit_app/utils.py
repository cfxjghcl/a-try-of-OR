# streamlit_app/utils.py
import streamlit as st
import pandas as pd
import json
import os, sys
import plotly.express as px
from collections import Counter
import re
from datetime import datetime, timezone # ç¡®ä¿å¯¼å…¥ timezone
import jieba # å¦‚æžœéœ€è¦ä¸­æ–‡åˆ†è¯
from itertools import combinations # ç”¨äºŽå…±çŽ°åˆ†æž
from collections import Counter
import pandas as pd
import numpy as np


# --- è·¯å¾„å®šä¹‰ ---
# __file__ is streamlit_app/utils.py
# streamlit_app_dir is the directory containing utils.py
streamlit_app_dir = os.path.dirname(os.path.abspath(__file__))
# PROJECT_ROOT is the parent of streamlit_app_dir
PROJECT_ROOT = os.path.dirname(streamlit_app_dir)

CRAWLER_MODULE_DIR = os.path.join(PROJECT_ROOT, 'crawler', 'crawler')
DATA_DIR_INSIDE_CRAWLER = os.path.join(CRAWLER_MODULE_DIR, 'data')

JOBS_FILE = os.path.join(DATA_DIR_INSIDE_CRAWLER, 'jobs.json') # æˆ– jobs.jsonl
CITIES_FILE = os.path.join(DATA_DIR_INSIDE_CRAWLER, 'cities.json') # Not actively used in provided snippets, but path is defined
CATEGORIES_FILE = os.path.join(DATA_DIR_INSIDE_CRAWLER, 'positions.json') # Used in Skills/Majors page
DEFAULT_OPTIONS_FILE_PATH = os.path.join(DATA_DIR_INSIDE_CRAWLER, 'target_options.json') # Path to target_options.json

# --- å¯é€‰ï¼šåŠ è½½åœç”¨è¯å’Œè‡ªå®šä¹‰è¯å…¸çš„è·¯å¾„ ---
ASSETS_DIR = os.path.join(streamlit_app_dir, 'assets') # assets folder inside streamlit_app
STOPWORDS_FILE = os.path.join(ASSETS_DIR, 'stopwords.txt')
USER_DICT_FILE = os.path.join(ASSETS_DIR, 'user_dict.txt')

# --- æŠ€èƒ½å…³é”®è¯åˆ—è¡¨ (æ ¸å¿ƒ) ---
DEFAULT_SKILL_KEYWORDS = [
    # ç¼–ç¨‹è¯­è¨€ & æ¡†æž¶
    'Python', 'Java', 'Go', 'Golang', 'C++', 'C#', 'JavaScript', 'JS', 'TypeScript', 'TS', 'PHP', 'Ruby', 'Swift', 'Kotlin', 'Scala', 'Rust', 'Perl',
    'React', 'React.js', 'Vue', 'Vue.js', 'Angular', 'Angular.js', 'Node.js', 'Node', 'Express.js',
    'Spring', 'Spring Boot', 'Django', 'Flask', 'FastAPI', 'Ruby on Rails', '.NET', 'ASP.NET',
    # æ•°æ®åº“
    'SQL', 'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQLServer', 'NoSQL', 'SQLite', 'Cassandra', 'Elasticsearch',
    # äº‘è®¡ç®— & DevOps
    'AWS', 'Azure', 'GCP', 'é˜¿é‡Œäº‘', 'è…¾è®¯äº‘', 'åŽä¸ºäº‘', 'äº‘è®¡ç®—', 'äº‘åŽŸç”Ÿ', 'Cloud', 'OpenStack', 'Serverless',
    'Docker', 'Kubernetes', 'K8S', 'CI/CD', 'DevOps', 'Jenkins', 'Git', 'GitLab', 'GitHub', 'Ansible', 'Terraform', 'Puppet', 'Chef',
    # æ“ä½œç³»ç»Ÿ & Shell
    'Linux', 'Unix', 'Windows Server', 'Shell', 'Bash', 'PowerShell',
    # å¤§æ•°æ®
    'æ•°æ®åˆ†æž', 'æ•°æ®æŒ–æŽ˜', 'å¤§æ•°æ®', 'Spark', 'Apache Spark', 'Hadoop', 'Flink', 'Apache Flink', 'Kafka', 'Apache Kafka',
    'Hive', 'HBase', 'Storm', 'Presto', 'ClickHouse', 'ETL',
    # AI & æœºå™¨å­¦ä¹ 
    'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'AI', 'äººå·¥æ™ºèƒ½', 'NLP', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'CV', 'è®¡ç®—æœºè§†è§‰', 'æŽ¨èç®—æ³•', 'TensorFlow', 'PyTorch', 'Keras', 'Scikit-learn',
    # å‰åŽç«¯ & æµ‹è¯• & è¿ç»´
    'ç®—æ³•', 'æž¶æž„', 'æž¶æž„å¸ˆ', 'ç³»ç»Ÿè®¾è®¡', 'å¾®æœåŠ¡',
    'å‰ç«¯', 'åŽç«¯', 'å…¨æ ˆ',
    'æµ‹è¯•', 'è½¯ä»¶æµ‹è¯•', 'è‡ªåŠ¨åŒ–æµ‹è¯•', 'æ€§èƒ½æµ‹è¯•', 'å®‰å…¨æµ‹è¯•', 'QA',
    'è¿ç»´', 'SRE', 'ç›‘æŽ§', 'æ—¥å¿—',
    # èŒä½è§’è‰² & è½¯æŠ€èƒ½ (éƒ¨åˆ†ä¹Ÿå¯èƒ½æ˜¯å…³é”®è¯)
    'äº§å“ç»ç†', 'é¡¹ç›®ç»ç†', 'PM', 'PO',
    'è¿è¥', 'å¸‚åœº', 'é”€å”®', 'BD', 'å®¢æœ',
    'HR', 'äººäº‹', 'æ‹›è˜',
    'è¡Œæ”¿', 'è´¢åŠ¡', 'ä¼šè®¡', 'å®¡è®¡', 'æ³•åŠ¡', 'é£ŽæŽ§',
    # è®¾è®¡ & æ¸¸æˆ
    'UI', 'UX', 'ç”¨æˆ·ä½“éªŒ', 'äº¤äº’è®¾è®¡', 'è§†è§‰è®¾è®¡', 'è®¾è®¡', 'å¹³é¢è®¾è®¡',
    'æ¸¸æˆå¼€å‘', 'UE4', 'UE5', 'Unreal Engine', 'Unity', 'U3D', 'Cocos',
    # ç§»åŠ¨å¼€å‘
    'ç§»åŠ¨å¼€å‘', 'iOS', 'Android', 'Flutter', 'React Native', 'å°ç¨‹åº', 'é¸¿è’™', 'HarmonyOS',
    # å…¶ä»–æŠ€æœ¯ & é¢†åŸŸ
    'åµŒå…¥å¼', 'ç‰©è”ç½‘', 'IoT', 'èŠ¯ç‰‡', 'FPGA', 'ASIC', 'é©±åŠ¨å¼€å‘', 'å›ºä»¶',
    'ç½‘ç»œå®‰å…¨', 'ä¿¡æ¯å®‰å…¨', 'å¯†ç å­¦', 'æ¸—é€æµ‹è¯•',
    'åŒºå—é“¾', 'æ™ºèƒ½åˆçº¦',
    'å†™ä½œ', 'ç¼–è¾‘', 'æ–‡æ¡ˆ', 'ç¿»è¯‘',
    'åŒ»å­¦', 'è¯å­¦', 'ç”Ÿç‰©', 'åŒ–å­¦',
    # é€šç”¨ä¸“ä¸š/é¢†åŸŸè¯ (ä¹Ÿå¯èƒ½å‡ºçŽ°åœ¨ major_required)
    'è®¡ç®—æœº', 'è½¯ä»¶å·¥ç¨‹', 'é€šä¿¡å·¥ç¨‹', 'ç”µå­ä¿¡æ¯', 'è‡ªåŠ¨åŒ–', 'æœºæ¢°', 'æ•°å­¦', 'ç»Ÿè®¡', 'ç‰©ç†', 'é‡‘èž', 'ç»æµŽ'
]

DEFAULT_COMPANY_TAGS = [
    "äº”é™©ä¸€é‡‘", "å¹´åº•åŒè–ª", "ç»©æ•ˆå¥–é‡‘", "å¸¦è–ªå¹´å‡", "å¼¹æ€§å·¥ä½œ",
    "å®šæœŸä½“æ£€", "è¡¥å……åŒ»ç–—ä¿é™©", "äº¤é€šè¡¥è´´", "é¤é¥®è¡¥è´´", "é€šè®¯è¡¥è´´",
    "èŠ‚æ—¥ç¦åˆ©", "ä¸“ä¸šåŸ¹è®­", "æ™‹å‡ç©ºé—´å¤§", "æ‰å¹³åŒ–ç®¡ç†", "æŠ€æœ¯æ°›å›´å¥½",
    "å›¢é˜Ÿä¼˜ç§€", "è‚¡ç¥¨æœŸæƒ", "å…è´¹ç­è½¦", "å¥èº«æˆ¿", "é›¶é£Ÿä¸‹åˆèŒ¶",
    "æä¾›ä½å®¿", "å¸¦è–ªå¹´å‡", "ç»©æ•ˆå¥–é‡‘", "èŠ‚æ—¥ç¤¼ç‰©", "å®šæœŸä½“æ£€", "åˆé¤è¡¥è´´"
    # ... add more standard tags
]

PREPROCESS_SCALES_ORDERED = ["1-49äºº", "50-99äºº", "100-499äºº", "500-999äºº", "1000-9999äºº", "10000+äºº", "æœªçŸ¥"]
PREPROCESS_DEGREES_ORDERED = ['å­¦åŽ†ä¸é™', 'å…¶ä»–', 'åˆä¸­åŠä»¥ä¸‹', 'ä¸­ä¸“', 'ä¸­æŠ€', 'ä¸­ä¸“/ä¸­æŠ€', 'é«˜ä¸­', 'å¤§ä¸“', 'æœ¬ç§‘', 'ç¡•å£«', 'åšå£«', 'åšå£«åŽ']
PREPROCESS_WORK_YEAR_LABELS = ['ç»éªŒä¸é™', '1å¹´ä»¥å†…', '1-3å¹´', '3-5å¹´', '5-10å¹´', '10å¹´ä»¥ä¸Š']

# å¯¹å…³é”®è¯è¿›è¡Œé¢„å¤„ç†ï¼Œä¾‹å¦‚å…¨éƒ¨è½¬ä¸ºå°å†™ï¼Œå¹¶åŽ»é‡ï¼Œä½†ä¿ç•™åŽŸå§‹å¤§å°å†™ç”¨äºŽæ˜ å°„
# æž„å»ºä¸€ä¸ªæ˜ å°„ï¼šå°å†™å…³é”®è¯ -> åŽŸå§‹å¤§å°å†™å…³é”®è¯ (é€‰æ‹©ç¬¬ä¸€ä¸ªå‡ºçŽ°çš„åŽŸå§‹å¤§å°å†™)
LOWER_TO_ORIGINAL_SKILL_MAP = {kw.lower(): kw for kw in reversed(DEFAULT_SKILL_KEYWORDS)} # reversedç¡®ä¿çŸ­è¯ä¸ä¼šè¦†ç›–é•¿è¯çš„å°å†™å½¢å¼
UNIQUE_LOWER_SKILLS = list(LOWER_TO_ORIGINAL_SKILL_MAP.keys())

# ä¸ºåŽ»é‡åŽçš„å°å†™æŠ€èƒ½æž„å»ºæ­£åˆ™è¡¨è¾¾å¼ (æŒ‰é•¿åº¦é™åºæŽ’åˆ—ï¼Œä¼˜å…ˆåŒ¹é…é•¿è¯)
SORTED_UNIQUE_LOWER_SKILLS_FOR_REGEX = sorted(UNIQUE_LOWER_SKILLS, key=len, reverse=True)

SKILL_REGEX_PATTERNS = {}
for lower_skill in SORTED_UNIQUE_LOWER_SKILLS_FOR_REGEX:
    original_kw = LOWER_TO_ORIGINAL_SKILL_MAP[lower_skill]
    pattern_str = ""
    # å¯¹ original_kw è¿›è¡Œ re.escape æ˜¯å¿…è¦çš„ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ç‰¹æ®Šå­—ç¬¦å¦‚ C++ æ—¶
    escaped_kw = re.escape(original_kw)
    if re.search(r'[\u4e00-\u9fff]', original_kw): # ä¸­æ–‡æˆ–ä¸­è‹±æ··åˆ
        pattern_str = escaped_kw # ç²¾ç¡®åŒ¹é…
    elif original_kw.isalnum() or (original_kw.isascii() and not any(c in '+*#.' for c in original_kw)): # ç®€å•è‹±æ–‡/æ•°å­— (é¿å…å¯¹C++, C#ç­‰ç”¨\b)
        pattern_str = r'\b' + escaped_kw + r'\b'
    else: # åŒ…å«ç‰¹æ®Šç¬¦å·çš„ï¼Œå¦‚ C++, C#, Node.js, React.js
        pattern_str = escaped_kw # ç²¾ç¡®åŒ¹é…ï¼Œå› ä¸º\bå¯èƒ½ä¸é€‚ç”¨
    SKILL_REGEX_PATTERNS[original_kw] = re.compile(pattern_str, re.IGNORECASE)

# CRAWLER_SETTINGS_FILE = os.path.join(PROJECT_ROOT, 'crawler', 'crawler', 'settings.py') # Not actively used for options loading

TAGS_TO_EXCLUDE_GENERIC = [
    'å…¶ä»–', 'å…¶å®ƒ', 'å…¬å¸æä¾›', 'å‘˜å·¥ç¦åˆ©', 'ç¦åˆ©å¾…é‡', 'å¾…é‡ä»Žä¼˜',
    'è¯¦æƒ…é¢è®®', 'é¢è®®', 'å·¥ä½œé¤', 'åŒ…åƒ', 'åŒ…ä½', 'åŒ…åƒä½', 'é£Ÿå®¿è¡¥è´´', 'ä½å®¿è¡¥è´´', # These can be too generic
    'ç­è½¦', 'äº¤é€šæ–¹ä¾¿', 'åœ°é“æ²¿çº¿', 'ä¸åŠ ç­', 'å¶å°”åŠ ç­', # These are good but maybe too many if not top-level
    # Add more generic or unhelpful tags here
]

# Compile a regex for characters to remove or replace in tags
NON_ALPHANUMERIC_CHINESE_REGEX = re.compile(r'[^\w\s\u4e00-\u9fff\+\#\.\-]') # Allows alphanumeric, whitespace, CJK, +, #, ., -

def clean_tag(tag):
    """Cleans an individual tag."""
    if not isinstance(tag, str):
        tag = str(tag)
    tag = tag.strip().lower() # Lowercase and strip whitespace
    tag = NON_ALPHANUMERIC_CHINESE_REGEX.sub('', tag) # Remove unwanted characters
    # Optional: Replace multiple spaces with a single space
    tag = re.sub(r'\s+', ' ', tag).strip()
    return tag

def get_cleaned_company_tags_from_data(df_jobs, top_n=50, min_freq=5):
    """
    Extracts, cleans, and gets frequent company tags from the 'tags_list' column.
    """
    if 'tags_list' not in df_jobs.columns:
        return []

    all_tags_flat = []
    for tag_tuple in df_jobs['tags_list'].dropna():
        if isinstance(tag_tuple, (list, tuple)):
            for tag in tag_tuple:
                cleaned = clean_tag(tag)
                if cleaned and cleaned not in TAGS_TO_EXCLUDE_GENERIC and len(cleaned) > 1 and len(cleaned) < 15: # Basic length filter
                    all_tags_flat.append(cleaned)

    if not all_tags_flat:
        return []

    tag_counts = Counter(all_tags_flat)
    
    # Get tags that meet min_freq and then take top_n from those
    frequent_tags = [tag for tag, count in tag_counts.items() if count >= min_freq]
    
    # If still too many after min_freq, take top_n based on original counts
    if len(frequent_tags) > top_n:
        # Sort all tags by frequency to pick the true top_n from those meeting min_freq
        # This ensures we don't just pick alphabetically if many have the same min_freq
        sorted_frequent_tags = sorted(frequent_tags, key=lambda t: tag_counts[t], reverse=True)
        return sorted_frequent_tags[:top_n]
    elif frequent_tags: # If fewer than top_n but some exist
        return sorted(frequent_tags, key=lambda t: tag_counts[t], reverse=True) # Sort them by freq
    
    # Fallback if min_freq filters out too much, just take top_n of all cleaned tags
    # This might happen with sparse data
    if not frequent_tags and tag_counts:
        print(f"Warning: min_freq={min_freq} for company tags filtered out all tags. Falling back to top_n of all cleaned tags.")
        return [tag for tag, count in tag_counts.most_common(top_n)]

    return []

# --- Helper function for major standardization ---
@st.cache_data
def _load_standard_majors(target_options_path=DEFAULT_OPTIONS_FILE_PATH):
    standard_majors_lower_sorted = []
    try:
        if os.path.exists(target_options_path):
            with open(target_options_path, 'r', encoding='utf-8') as f:
                target_options_data = json.load(f)
            
            college_major_list = target_options_data.get('collegmajor') 
            
            if isinstance(college_major_list, list):
                raw_majors = []
                for item in college_major_list:
                    if isinstance(item, dict) and 'name' in item and isinstance(item['name'], str):
                        raw_majors.append(item['name'])
                
                if raw_majors:
                    valid_majors = [m.lower() for m in raw_majors if m.strip()]
                    standard_majors_lower_sorted = sorted(list(set(valid_majors)), key=len, reverse=True)
                    if not standard_majors_lower_sorted and streamlit_app_dir: # Check if in Streamlit context
                        st.warning(f"'{os.path.basename(target_options_path)}' yielded no valid major names after processing.")
                elif streamlit_app_dir:
                    st.warning(f"No major names extracted from 'collegmajor' in '{os.path.basename(target_options_path)}'.")
            elif streamlit_app_dir:
                st.warning(f"'collegmajor' key in '{os.path.basename(target_options_path)}' is not a list or is missing.")
        elif streamlit_app_dir:
            st.warning(f"Standard majors file '{os.path.basename(target_options_path)}' not found. Major processing will be limited.")
    except json.JSONDecodeError:
        if streamlit_app_dir: st.error(f"Error decoding JSON from '{os.path.basename(target_options_path)}'.")
    except Exception as e:
        if streamlit_app_dir: st.error(f"Error loading majors from '{os.path.basename(target_options_path)}': {e}")
    
    return standard_majors_lower_sorted

STOPWORDS = {"çš„", "ä¸Ž", "å’Œ", "ç­‰", "åŠ", "ä¸­", "å…·æœ‰", "èƒ½åŠ›", "ç»éªŒ", "è´Ÿè´£", "ç†Ÿç»ƒ", "æŽŒæ¡", "è¿›è¡Œ", "ç›¸å…³", "ä¼˜å…ˆ", "è‰¯å¥½"} # Add more

def extract_skills_advanced(text_series, predefined_skills=None, min_skill_len=2):
    if predefined_skills is None:
        predefined_skills = set() # PREDEFINED_SKILLS loaded globally or passed

    def extract_from_text(text):
        if pd.isna(text) or not isinstance(text, str) or not text.strip():
            return tuple()
        
        text_lower = text.lower()
        found_skills = set()

        # 1. Match predefined skills (case-insensitive)
        for skill in predefined_skills:
            if skill in text_lower: # Simple substring match for predefined
                found_skills.add(skill.strip()) # Use original casing from dict if desired, or always lower

        # 2. Use Jieba for other terms (optional, can be noisy without good post-filtering)
        # words = jieba.lcut(text)
        # for word in words:
        #     word_cleaned = word.strip().lower()
        #     if len(word_cleaned) >= min_skill_len and word_cleaned not in STOPWORDS and not word_cleaned.isnumeric():
        #         # Add more filtering here: e.g., check if it's a noun, part of an N-gram, etc.
        #         # This part can become complex to get good results without a dictionary
        #         if any(char.isalpha() for char in word_cleaned): # crude filter for things that might be skills
        #             found_skills.add(word_cleaned)
        
        return tuple(sorted(list(found_skills)))

    return text_series.apply(extract_from_text)

def get_word_counts_from_list_column(df, column_name, top_n=None, min_freq=1): # Added min_freq with default
    """
    Counts occurrences of items within a column containing lists/tuples of items.
    Filters by minimum frequency before selecting top N.
    """
    if column_name not in df.columns:
        # st.warning(f"Column '{column_name}' not found in DataFrame.") # Optional warning
        return pd.DataFrame(columns=['item', 'count'])

    # Ensure the column is not all NaNs or empty lists/tuples
    if df[column_name].isnull().all() or not df[column_name].apply(lambda x: isinstance(x, (list, tuple)) and len(x) > 0).any():
        # st.info(f"Column '{column_name}' is empty or contains no valid lists/tuples of items.") # Optional info
        return pd.DataFrame(columns=['item', 'count'])

    all_items = []
    for item_list in df[column_name].dropna(): # Drop NaNs before iterating
        if isinstance(item_list, (list, tuple)):
            all_items.extend([item for item in item_list if isinstance(item, str) and item.strip()]) # Add only non-empty strings
        # elif isinstance(item_list, str): # If some rows have single strings instead of lists
            # all_items.append(item_list) 
            
    if not all_items:
        return pd.DataFrame(columns=['item', 'count'])

    item_counts = Counter(all_items)
    
    # Filter by min_freq
    filtered_counts = {item: count for item, count in item_counts.items() if count >= min_freq}
    
    if not filtered_counts:
        return pd.DataFrame(columns=['item', 'count'])

    # Convert to DataFrame
    df_counts = pd.DataFrame(list(filtered_counts.items()), columns=['item', 'count'])
    df_counts = df_counts.sort_values(by='count', ascending=False).reset_index(drop=True)

    if top_n:
        return df_counts.head(top_n)
    return df_counts

def _map_to_standard_major(major_text, standard_majors_list):
    """
    Maps a raw major text to a standard major from the list.
    major_text: Raw major string, expected to be lowercased and stripped.
    standard_majors_list: List of standard majors, lowercased, sorted by length desc.
    """
    if not major_text or major_text == 'æœªçŸ¥': 
        return 'æœªçŸ¥'
    
    for std_major in standard_majors_list:
        if std_major in major_text: 
            return std_major 
    return 'å…¶ä»–ä¸“ä¸š' 
# --- End of Helper function for major standardization ---


@st.cache_data
def load_scrapy_default_targets():
    # å®šä¹‰æ‰€æœ‰æœŸæœ›çš„é”®å’Œå®ƒä»¬å¯¹åº”çš„ "ä¸é™" é€‰é¡¹çš„æ˜¾ç¤ºåç§°åŠé»˜è®¤ç»“æž„
    default_options_config = {
        "cities": {"default_name": "ðŸŒ ä¸é™çœä»½/åœ°åŒº", "json_keys": ["provinces", "citys"], "options": []},
        "categories": {"default_name": "ðŸ“š ä¸é™ç±»åˆ«", "json_keys": ["jobcategoryItems", "categories"], "options": []},
        "industries": {"default_name": "ðŸ­ ä¸é™è¡Œä¸š", "json_keys": ["industriesNew", "mainindustries"], "options": []},
        "workExperiences": {
            "default_name": "â³ ä¸é™å·¥ä½œç»éªŒ",
            "json_keys": ["workExperiences"],
            "options": [{"name": label, "code": str(idx)} for idx, label in enumerate(PREPROCESS_WORK_YEAR_LABELS)]
        },
        "degrees": {
            "default_name": "ðŸŽ“ ä¸é™å­¦åŽ†",
            "json_keys": ["degrees", "educationLevels"],
            "options": [{"name": label, "code": str(idx)} for idx, label in enumerate(PREPROCESS_DEGREES_ORDERED)]
        },
        "scales": {
            "default_name": "âš–ï¸ ä¸é™å…¬å¸è§„æ¨¡",
            "json_keys": ["scales", "companyScales"],
             # ä½¿ç”¨ PREPROCESS_SCALES_ORDEREDï¼Œä½†ç§»é™¤ "æœªçŸ¥" å› ä¸ºç”¨æˆ·é€šå¸¸ä¸æŒ‰ "æœªçŸ¥" ç­›é€‰
            "options": [{"name": label, "code": str(idx)} for idx, label in enumerate(cat for cat in PREPROCESS_SCALES_ORDERED if cat != "æœªçŸ¥")]
        },
        "corpProps": { # è¿™ä¸ªåœ¨ä½ çš„ JSON é‡Œæœ‰
            "default_name": "ðŸ›ï¸ ä¸é™å…¬å¸æ€§è´¨",
            "json_keys": ["corpProps"],
            "options": [] # å°†ç”± JSON æ–‡ä»¶å¡«å……
        }
    }

    targets = {}
    # 1. ä¸ºæ‰€æœ‰æœŸæœ›çš„é”®ä½¿ç”¨ default_options_config ä¸­çš„é…ç½®è¿›è¡Œåˆå§‹åŒ–
    for key, config in default_options_config.items():
        # åˆå§‹åˆ—è¡¨åŒ…å« "ä¸é™" é€‰é¡¹
        current_key_options = [{"code": "", "name": config["default_name"]}]
        # å¦‚æžœ config ä¸­æœ‰é¢„å®šä¹‰çš„ fallback optionsï¼Œè¿½åŠ å®ƒä»¬
        if config["options"]:
            # é¿å…é‡å¤æ·»åŠ ä¸Ž "ä¸é™" åç§°ç›¸åŒçš„é¡¹ï¼ˆå¦‚æžœfallback optionsé‡ŒåŒ…å«äº†å®ƒï¼‰
            for option_item in config["options"]:
                if option_item["name"] != config["default_name"]:
                    current_key_options.append(option_item)
        targets[key] = current_key_options

    # 2. å°è¯•ä»Ž target_options.json åŠ è½½å¹¶è¦†ç›–/åˆå¹¶
    if os.path.exists(DEFAULT_OPTIONS_FILE_PATH):
        try:
            with open(DEFAULT_OPTIONS_FILE_PATH, 'r', encoding='utf-8') as f:
                loaded_options_from_file = json.load(f)

            for key, config in default_options_config.items():
                loaded_list_for_key = None
                for json_key_attempt in config["json_keys"]:
                    if json_key_attempt in loaded_options_from_file and \
                       isinstance(loaded_options_from_file[json_key_attempt], list) and \
                       loaded_options_from_file[json_key_attempt]:
                        loaded_list_for_key = loaded_options_from_file[json_key_attempt]
                        print(f"Info: Successfully loaded '{json_key_attempt}' for key '{key}' from JSON.")
                        break
                
                if loaded_list_for_key:
                    valid_items_from_json = []
                    # JSON ä¸­çš„ "ä¸é™" é€‰é¡¹æ˜¯å¦å·²æ‰¾åˆ°å¹¶å¤„ç†ï¼ˆä»¥é¿å…é‡å¤æ·»åŠ ï¼‰
                    default_option_handled_from_json = False

                    for item_json in loaded_list_for_key:
                        if isinstance(item_json, dict) and item_json.get('name') and 'code' in item_json:
                            name_val = str(item_json['name']).strip()
                            code_val = str(item_json['code'])
                            
                            # å¦‚æžœJSONé¡¹æ˜¯ "ä¸é™" é€‰é¡¹
                            if name_val == config["default_name"]:
                                # ä½¿ç”¨JSONä¸­çš„codeï¼ˆå¦‚æžœå®ƒä¸æ˜¯ç©ºï¼‰ï¼Œå¦åˆ™ä¿æŒcodeä¸º""
                                valid_items_from_json.insert(0, {"name": name_val, "code": code_val if code_val else ""})
                                default_option_handled_from_json = True
                                continue # è·³è¿‡ï¼Œé¿å…é‡å¤æ·»åŠ åˆ°æœ«å°¾

                            processed_item = None
                            if key == "cities" and "citys" in config["json_keys"] and "level" in item_json:
                                if str(item_json.get("level")).lower() in ["çœ", "ç›´è¾–å¸‚", "è‡ªæ²»åŒº", "ç‰¹åˆ«è¡Œæ”¿åŒº"]:
                                     processed_item = {"name": name_val, "code": code_val}
                            # å¯¹äºŽ corpPropsï¼Œå…¶ JSON ä¸­çš„ "ä¸é™" å¯èƒ½æœ‰ codeï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰
                            elif key == "corpProps" and name_val == "ä¸é™" and code_val == "": # è¿™æ˜¯ utils/app.py æœŸæœ›çš„
                                valid_items_from_json.insert(0, {"name": config["default_name"], "code": ""}) # ä½¿ç”¨è§„èŒƒçš„ "ä¸é™"
                                default_option_handled_from_json = True
                                continue
                            else:
                                processed_item = {"name": name_val, "code": code_val}
                            
                            if processed_item:
                                valid_items_from_json.append(processed_item)
                    
                    if valid_items_from_json:
                        # å¦‚æžœJSONåŠ è½½æˆåŠŸï¼Œç”¨JSONçš„æ•°æ®ï¼ˆä½†ç¡®ä¿ "ä¸é™" åœ¨æœ€å‰é¢ä¸”è§„èŒƒï¼‰
                        final_list_for_key = []
                        if not default_option_handled_from_json:
                            final_list_for_key.append({"code": "", "name": config["default_name"]})
                        
                        # æ·»åŠ JSONä¸­éž "ä¸é™" çš„é¡¹ï¼Œå¹¶åŽ»é‡ï¼ˆåŸºäºŽnameï¼‰
                        seen_names = {config["default_name"]} if default_option_handled_from_json else set()
                        if default_option_handled_from_json and valid_items_from_json[0]["name"] == config["default_name"]:
                            final_list_for_key.append(valid_items_from_json[0]) # æ·»åŠ å·²å¤„ç†çš„ JSON "ä¸é™"
                            start_index_json = 1
                        else:
                            start_index_json = 0

                        for vi in valid_items_from_json[start_index_json:]:
                            if vi["name"] not in seen_names:
                                final_list_for_key.append(vi)
                                seen_names.add(vi["name"])
                        targets[key] = final_list_for_key
                # å¦‚æžœ loaded_list_for_key ä¸º None æˆ–å¤„ç†åŽ valid_items_from_json ä¸ºç©ºï¼Œ
                # targets[key] ä¼šä¿æŒå…¶åˆå§‹åŒ–çš„å€¼ (åŒ…å« "ä¸é™" å’Œå¯èƒ½çš„ç¡¬ç¼–ç  fallback options)
        
        except json.JSONDecodeError:
            msg = f"è§£æž JSON æ–‡ä»¶ '{os.path.basename(DEFAULT_OPTIONS_FILE_PATH)}' å¤±è´¥ã€‚å°†ä½¿ç”¨å†…ç½®é»˜è®¤çˆ¬å–é€‰é¡¹ã€‚"
            if 'streamlit' in sys.modules and hasattr(st, 'warning'): st.warning(msg)
            else: print(f"Warning: {msg}")
        except Exception as e:
            msg = f"åŠ è½½çˆ¬å–é€‰é¡¹æ—¶å‘ç”Ÿé”™è¯¯: {e}ã€‚å°†ä½¿ç”¨å†…ç½®é»˜è®¤çˆ¬å–é€‰é¡¹ã€‚"
            if 'streamlit' in sys.modules and hasattr(st, 'warning'): st.warning(msg)
            else: print(f"Warning: {msg}")
    else:
        msg = f"çˆ¬å–é€‰é¡¹æ–‡ä»¶ '{os.path.basename(DEFAULT_OPTIONS_FILE_PATH)}' æœªæ‰¾åˆ°ã€‚å°†ä½¿ç”¨å†…ç½®é»˜è®¤çˆ¬å–é€‰é¡¹ã€‚"
        if 'streamlit' in sys.modules and hasattr(st, 'warning'): st.warning(msg)
        else: print(f"Warning: {msg}")
        # æ­¤æ—¶ targets å­—å…¸å·²ç»åŒ…å«äº†æ‰€æœ‰é”®çš„é»˜è®¤ "ä¸é™" + fallback options

    # 3. æœ€åŽï¼Œå†æ¬¡ç¡®ä¿æ¯ä¸ªåˆ—è¡¨çš„ "ä¸é™" é€‰é¡¹æ˜¯å”¯ä¸€çš„ï¼Œå¹¶ä¸”æ ¼å¼æ­£ç¡®ï¼Œä¸”ä½äºŽæœ€å‰
    for key, config in default_options_config.items():
        current_list = targets.get(key, []) 
        
        final_unique_list = []
        default_option = {"code": "", "name": config["default_name"]}
        
        final_unique_list.append(default_option) # å¼ºåˆ¶ "ä¸é™" åœ¨ç¬¬ä¸€ä½
        
        seen_names_in_final = {config["default_name"]} # è®°å½•å·²åŠ å…¥ final_list çš„ name

        for item in current_list:
            item_name = item.get("name")
            # è·³è¿‡ä¸Žå·²æ·»åŠ çš„è§„èŒƒ "ä¸é™" é€‰é¡¹ name ç›¸åŒçš„é¡¹ (å› ä¸º "ä¸é™" å·²è¢«å¼ºåˆ¶åŠ å…¥)
            if item_name == config["default_name"]:
                continue
            
            if item_name and item_name not in seen_names_in_final:
                 # ç¡®ä¿ item æ˜¯å­—å…¸ä¸”æœ‰ code
                if isinstance(item, dict) and 'code' in item:
                    final_unique_list.append(item)
                    seen_names_in_final.add(item_name)
        
        targets[key] = final_unique_list
        
    return targets

@st.cache_data
def load_stopwords(filepath=STOPWORDS_FILE):
    stopwords = set()
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                stopwords.add(line.strip())
    else:
        stopwords = {"çš„", "äº†", "å’Œ", "ä¸Ž", "æˆ–", "ä¹Ÿ", "ç­‰", "åœ¨", "æ˜¯", "æˆ‘ä»¬", "ä»¥åŠ", " ", "\n", "\t",
                     "å…¬å¸", "æœ‰é™", "ç§‘æŠ€", "è‚¡ä»½", "é›†å›¢", "ä¼ä¸š", "åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "å¹¿å·ž", 
                     "è¦æ±‚", "è´Ÿè´£", "ç›¸å…³", "å·¥ä½œ", "ç»éªŒ", "ä¼˜å…ˆ", "èƒ½åŠ›", "ç†Ÿæ‚‰", "æŽŒæ¡", "å²—ä½", "èŒè´£"}
    return stopwords

def parse_and_map_majors_from_text_util(text, official_names_set_local): # Renamed to avoid conflict if copied
    if pd.isna(text) or not isinstance(text, str): return tuple()
    text_cleaned = str(text).strip().lower()
    no_specific_major_phrases = ['ä¸é™ä¸“ä¸š', 'ä¸é™', 'ä¸“ä¸šä¸é™', 'æ— ', 'ç›¸å…³ä¸“ä¸š', 'ä¸è¦æ±‚']
    if not text_cleaned or any(phrase in text_cleaned for phrase in no_specific_major_phrases):
        return tuple()
    text_cleaned_no_prefix = re.sub(r'ã€.*?ã€‘', '', text_cleaned) 
    text_cleaned_no_prefix = re.sub(r'ï¼ˆ.*?ï¼‰', '', text_cleaned_no_prefix) 
    text_cleaned_no_prefix = re.sub(r'\(.*?\)', '', text_cleaned_no_prefix)
    found_official_majors = set()
    for official_major in official_names_set_local:
        if re.search(r'\b' + re.escape(official_major) + r'\b', text_cleaned_no_prefix):
            found_official_majors.add(official_major)
    if not found_official_majors:
        delimiters = r'[ã€,\sï¼Œï¼›æˆ–/åŠ]+' 
        parts = [p.strip() for p in re.split(delimiters, text_cleaned_no_prefix) if p.strip()]
        for part in parts:
            if part in official_names_set_local:
                found_official_majors.add(part)
    return tuple(sorted(list(found_official_majors)))

# --- REMOVE THE FIRST preprocess_jobs_data DEFINITION ---
# # --- Your existing preprocess_jobs_data function ---
# @st.cache_data
# def preprocess_jobs_data(df_jobs_raw): # ç¡®ä¿è¿™ä¸ªå‡½æ•°åœ¨è¿™é‡Œ
#     if df_jobs_raw.empty:
#         return pd.DataFrame()
#     df = df_jobs_raw.copy()

#     # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åˆ—éƒ½å­˜åœ¨ï¼Œå¹¶è¿›è¡Œç±»åž‹è½¬æ¢å’Œå¡«å……
#     required_cols_defaults = {
#         'job_id': None, 'job_name': 'æœªçŸ¥èŒä½', 'job_catory': 'æœªçŸ¥ç±»åˆ«', 'job_industry': 'æœªçŸ¥è¡Œä¸š',
#         'high_month_pay': 0.0, 'low_month_pay': 0.0, 'publish_date': None, 'update_date': None,
#         'company_name': 'æœªçŸ¥å…¬å¸',
#         'area_code_name': 'æœªçŸ¥åœ°åŒº',
#         'prinvce_code_nme': 'æœªçŸ¥çœä»½',
#         'company_scale': 'æœªçŸ¥è§„æ¨¡',
#         'degree_name': 'ä¸é™', 'major_required': '', 'company_property': 'æœªçŸ¥æ€§è´¨',
#         'company_tags': '', 'source_url': '#', 'head_count': 1
#     }
#     for col, default_val in required_cols_defaults.items():
#         if col not in df.columns:
#             df[col] = default_val
#         elif col in ['major_required', 'company_tags', 'job_name', 'job_catory', 'job_industry',
#                      'company_name', 'area_code_name', 'prinvce_code_nme', 'company_scale',
#                      'degree_name', 'company_property']:
#             if default_val is not None:
#                  df[col] = df[col].fillna(str(default_val))
#             df[col] = df[col].astype(str)

#     for col in ['low_month_pay', 'high_month_pay']:
#         df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

#     df['avg_month_pay'] = df.apply(
#         lambda row: (row['low_month_pay'] + row['high_month_pay']) / 2 if row['low_month_pay'] > 0 and row['high_month_pay'] > 0 else \
#                     row['high_month_pay'] if row['high_month_pay'] > 0 else \
#                     row['low_month_pay'] if row['low_month_pay'] > 0 else 0.0,
#         axis=1
#     )

#     for date_col in ['publish_date', 'update_date']:
#         if date_col in df.columns:
#             df[date_col] = pd.to_numeric(df[date_col], errors='coerce')
#             df[f'{date_col}_dt'] = pd.to_datetime(df[date_col], unit='ms', errors='coerce', utc=True)
    
#     location_suffixes_to_remove = ['å¸‚', 'çœ', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº', 'åœ°åŒº', 'ç›Ÿ', 'å·ž', 'åŽ¿', 'åŒº', 'å›žæ—', 'ç»´å¾å°”', 'å£®æ—', 'è—æ—', 'è‹—æ—', 'åœŸå®¶æ—', 'å¸ƒä¾æ—', 'ä¾—æ—', 'ç‘¶æ—', 'ç™½æ—', 'å“ˆå°¼æ—', 'å‚£æ—', 'å‚ˆåƒ³æ—', 'å½æ—']
#     temp_city_clean = df['area_code_name'].astype(str)
#     for suffix in location_suffixes_to_remove:
#         temp_city_clean = temp_city_clean.str.replace(suffix, '', regex=False)
#     df['city_clean'] = temp_city_clean.str.strip()
#     city_empty_mask = (df['city_clean'] == '') | (df['area_code_name'] == required_cols_defaults['area_code_name'])
#     df.loc[city_empty_mask, 'city_clean'] = df.loc[city_empty_mask, 'area_code_name'].astype(str).str.strip()
#     df.loc[df['city_clean'] == '', 'city_clean'] = required_cols_defaults['area_code_name']

#     temp_province_clean = df['prinvce_code_nme'].astype(str)
#     for suffix in location_suffixes_to_remove:
#         temp_province_clean = temp_province_clean.str.replace(suffix, '', regex=False)
#     df['province_clean'] = temp_province_clean.str.strip()
#     province_empty_mask = (df['province_clean'] == '') | (df['prinvce_code_nme'] == required_cols_defaults['prinvce_code_nme'])
#     df.loc[province_empty_mask, 'province_clean'] = df.loc[province_empty_mask, 'prinvce_code_nme'].astype(str).str.strip()
#     df.loc[df['province_clean'] == '', 'province_clean'] = required_cols_defaults['prinvce_code_nme']
    
#     def clean_scale(scale_str):
#         if pd.isna(scale_str) or not isinstance(scale_str, str): return "æœªçŸ¥"
#         scale_str = scale_str.strip()
#         if not scale_str or scale_str.lower() in ['nan', 'none', 'null', 'æœªçŸ¥']: return "æœªçŸ¥"
#         if "ä»¥ä¸Š" in scale_str:
#             match = re.search(r'(\d+)', scale_str)
#             return f"{match.group(1)}+äºº" if match else scale_str
#         return scale_str
#     df['company_scale_cleaned'] = df['company_scale'].apply(clean_scale)
#     scale_order = ["1-49äºº", "50-99äºº", "100-499äºº", "500-999äºº", "1000-9999äºº", "10000+äºº", "æœªçŸ¥"]
#     df['company_scale_cat'] = pd.Categorical(df['company_scale_cleaned'], categories=scale_order, ordered=True)

#     degree_order = ['ä¸é™', 'åˆä¸­åŠä»¥ä¸‹','ä¸­ä¸“/ä¸­æŠ€','é«˜ä¸­','å¤§ä¸“','æœ¬ç§‘','ç¡•å£«','åšå£«','åšå£«åŽ','æœ¬ç§‘åŠä»¥ä¸Š', 'ç¡•å£«åŠä»¥ä¸Š','å­¦åŽ†ä¸é™']
#     df['degree_name_cat'] = pd.Categorical(df['degree_name'].fillna('ä¸é™').astype(str), categories=degree_order, ordered=True)

#     common_none_terms = ['none', 'null', 'æ— ', '']
#     standard_majors_list = _load_standard_majors()
#     df['major_required_cleaned'] = df['major_required'].astype(str).str.lower().str.strip()
#     df['major_required_cleaned'].replace(common_none_terms + ['ä¸é™', 'æœªçŸ¥'], 'æœªçŸ¥', inplace=True)
#     if standard_majors_list:
#         df['processed_major'] = df['major_required_cleaned'].apply(
#             lambda x: _map_to_standard_major(x, standard_majors_list)
#         )
#     else:
#         df['processed_major'] = df['major_required_cleaned'].apply(lambda x: 'å…¶ä»–ä¸“ä¸š' if x and x != 'æœªçŸ¥' else 'æœªçŸ¥')

#     def split_raw_majors(text):
#         if not text or text.lower().strip() in common_none_terms + ['ä¸é™', 'æœªçŸ¥']: return tuple()
#         text_cleaned = text.replace("æˆ–ç›¸å…³ä¸“ä¸š", " ç›¸å…³ä¸“ä¸š").replace("åŠç›¸å…³ä¸“ä¸š", " ç›¸å…³ä¸“ä¸š")
#         delimiters = r"[\s,ï¼Œã€;/]+"
#         raw_majors = [m.strip() for m in re.split(delimiters, text_cleaned) if m.strip() and m.strip().lower() not in common_none_terms and len(m.strip()) > 1]
#         return tuple(m for m in raw_majors if m)
#     df['majors_list'] = df['major_required'].apply(split_raw_majors)

#     df['company_tags'] = df['company_tags'].astype(str).fillna('')
#     df['tags_list'] = df['company_tags'].apply(
#         lambda x: tuple(t.strip() for t in x.split('ï¼Œ') if t.strip() and t.strip().lower() not in common_none_terms)
#         if x.strip() and x.strip().lower() not in common_none_terms else tuple()
#     )
    
#     # --- åˆ›å»ºç”¨äºŽæŠ€èƒ½æå–çš„ç»„åˆæ–‡æœ¬åˆ— ---
#     df['job_name_and_major_text'] = df['job_name'].fillna('') + " " + df['major_required'].fillna('')
#     # å¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸Š job_description (å¦‚æžœä½ çš„ JSON ä¸­æœ‰è¿™ä¸ªå­—æ®µçš„è¯)
#     # if 'job_description' in df.columns:
#     #     df['job_name_and_major_text'] += " " + df['job_description'].fillna('')
        
#     return df
# --- END OF REMOVAL ---


@st.cache_data
def extract_skills_from_text_series(
    text_series: pd.Series,
    skill_regex_patterns: dict # ä¼ å…¥é¢„ç¼–è¯‘çš„ skill_original_case -> regex_pattern æ˜ å°„
) -> pd.Series:
    """
    ä»Žç»™å®šçš„æ–‡æœ¬ Series ä¸­æå–æŠ€èƒ½ã€‚
    è¿”å›žä¸€ä¸ª Seriesï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªåŒ…å«è¯¥è¡Œæ–‡æœ¬ä¸­è¯†åˆ«å‡ºçš„æŠ€èƒ½ï¼ˆåŽŸå§‹å¤§å°å†™ï¼‰çš„å…ƒç»„ã€‚
    """
    if text_series.empty:
        return pd.Series([tuple() for _ in range(len(text_series))], index=text_series.index, dtype=object)

    def find_skills_in_one_text(text_content: str):
        if pd.isna(text_content) or not text_content.strip():
            return tuple()
        
        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼è¿›è¡ŒåŒ¹é…
        # è¿™é‡Œçš„ skill_regex_patterns åº”è¯¥æ˜¯ {original_skill_name: compiled_regex_object}
        found_skills_set = set()
        for original_skill, compiled_pattern in skill_regex_patterns.items():
            if compiled_pattern.search(text_content): # re.IGNORECASE å·²åœ¨ç¼–è¯‘æ—¶è®¾ç½®
                found_skills_set.add(original_skill)
        return tuple(sorted(list(found_skills_set)))

    return text_series.astype(str).apply(find_skills_in_one_text)


@st.cache_data
def get_skill_frequency(
    df_with_extracted_skills: pd.DataFrame, # DataFrame åŒ…å«ä¸€ä¸ªæŠ€èƒ½å…ƒç»„åˆ—è¡¨çš„åˆ—
    skill_list_column: str, # å­˜æŠ€èƒ½å…ƒç»„åˆ—è¡¨çš„åˆ—åï¼Œä¾‹å¦‚ 'extracted_skills'
    top_n: int = 20
) -> pd.DataFrame:
    """
    è®¡ç®—æŠ€èƒ½åˆ—è¡¨ä¸­å„é¡¹æŠ€èƒ½çš„å‡ºçŽ°é¢‘çŽ‡ã€‚
    """
    if df_with_extracted_skills.empty or skill_list_column not in df_with_extracted_skills.columns:
        return pd.DataFrame(columns=['skill', 'count'])

    all_skills_flat_list = []
    for skills_tuple in df_with_extracted_skills[skill_list_column].dropna():
        if isinstance(skills_tuple, tuple) and skills_tuple: # ç¡®ä¿æ˜¯å…ƒç»„ä¸”éžç©º
            all_skills_flat_list.extend(list(skills_tuple))
            
    if not all_skills_flat_list:
        return pd.DataFrame(columns=['skill', 'count'])
        
    skill_counts = Counter(all_skills_flat_list)
    top_skills = skill_counts.most_common(top_n)
    
    return pd.DataFrame(top_skills, columns=['skill', 'count'])


@st.cache_data
def get_skill_cooccurrence_optimized(
    df_with_extracted_skills: pd.DataFrame, # DataFrame åŒ…å«ä¸€ä¸ªæŠ€èƒ½å…ƒç»„åˆ—è¡¨çš„åˆ—
    skill_list_column: str, # å­˜æŠ€èƒ½å…ƒç»„åˆ—è¡¨çš„åˆ—åï¼Œä¾‹å¦‚ 'extracted_skills'
    top_n_cooc_pairs: int = 15,
    min_cooc_frequency: int = 2,
    min_skills_in_one_text: int = 2 # è¿™ä¸ªå‚æ•°çŽ°åœ¨ç”± skill_list_column çš„å†…å®¹é•¿åº¦å†³å®š
) -> pd.DataFrame:
    """
    è®¡ç®—æŠ€èƒ½åˆ—è¡¨ä¸­å„é¡¹æŠ€èƒ½çš„å…±çŽ°é¢‘çŽ‡ã€‚
    df_with_extracted_skills: DataFrame, å…¶ä¸­ä¸€åˆ— (skill_list_column) åŒ…å«å·²æå–çš„æŠ€èƒ½å…ƒç»„ã€‚
    """
    if df_with_extracted_skills.empty or skill_list_column not in df_with_extracted_skills.columns:
        return pd.DataFrame(columns=['skill_pair', 'count'])

    cooccurrence_counts = Counter()
    processed_texts_count = 0
    texts_meeting_min_skills_criteria = 0

    for skills_tuple in df_with_extracted_skills[skill_list_column].dropna():
        processed_texts_count +=1
        if isinstance(skills_tuple, tuple) and len(skills_tuple) >= min_skills_in_one_text:
            texts_meeting_min_skills_criteria +=1
            # æŠ€èƒ½å·²ç»æ˜¯åŽŸå§‹å¤§å°å†™ï¼Œå¹¶ä¸”å·²åŽ»é‡ (å› ä¸º extract_skills_from_text_series è¿”å›ž setè½¬æ¢çš„å…ƒç»„)
            # ç›´æŽ¥å¯¹å…ƒç»„å†…çš„æŠ€èƒ½è¿›è¡Œç»„åˆ
            for combo in combinations(sorted(list(skills_tuple)), 2): # sortedç¡®ä¿é¡ºåºä¸€è‡´
                cooccurrence_counts[combo] += 1
    
    # Debugging (å¯ä»¥åœ¨ Streamlit é¡µé¢ä¹‹å¤–æˆ–ä¸´æ—¶å–æ¶ˆæ³¨é‡Š)
    # print(f"DEBUG cooc_opt (from list col): Processed {processed_texts_count} skill lists.")
    # print(f"DEBUG cooc_opt (from list col): {texts_meeting_min_skills_criteria} lists had >= {min_skills_in_one_text} skills.")
    # print(f"DEBUG cooc_opt (from list col): Total unique co-occurring pairs found (before filtering): {len(cooccurrence_counts)}")
    # if cooccurrence_counts:
    #     print(f"DEBUG cooc_opt (from list col): Top 5 raw cooccurrence_counts: {dict(cooccurrence_counts.most_common(5))}")

    if not cooccurrence_counts:
        return pd.DataFrame(columns=['skill_pair', 'count'])

    frequent_cooccurrences = {
        pair: count for pair, count in cooccurrence_counts.items() if count >= min_cooc_frequency
    }
    if not frequent_cooccurrences:
        return pd.DataFrame(columns=['skill_pair', 'count'])

    top_common_combos = Counter(frequent_cooccurrences).most_common(top_n_cooc_pairs)
    if not top_common_combos:
        return pd.DataFrame(columns=['skill_pair', 'count'])

    combo_df = pd.DataFrame(top_common_combos, columns=['skill_pair_tuple', 'count'])
    combo_df['skill_pair'] = combo_df['skill_pair_tuple'].apply(lambda x: f"{x[0]} & {x[1]}")
    
    return combo_df[['skill_pair', 'count']].sort_values(by='count', ascending=False)


@st.cache_data
def extract_terms_jieba(text_series, top_n=25, custom_dict_path=USER_DICT_FILE, stopwords_path=STOPWORDS_FILE, min_len=2, min_freq=2):
    if text_series.empty:
        return pd.DataFrame(columns=['term', 'count'])

    if custom_dict_path and os.path.exists(custom_dict_path):
        try:
            jieba.load_userdict(custom_dict_path)
        except Exception as e:
            if streamlit_app_dir: st.warning(f"Could not load user dictionary: {e}")
    
    stopwords = load_stopwords(stopwords_path)
    all_words = []
    
    full_text = " ".join(text_series.dropna().astype(str).tolist())
    if not full_text.strip():
        return pd.DataFrame(columns=['term', 'count'])

    words = jieba.lcut(full_text, cut_all=False)
    for word in words:
        word = word.strip().lower()
        if word and word not in stopwords and len(word) >= min_len and \
           not word.isdigit() and not (len(word)==1 and 'a' <= word <= 'z'):
            all_words.append(word)
    
    if not all_words:
        return pd.DataFrame(columns=['term', 'count'])
        
    word_counts = Counter(all_words)
    if min_freq > 1:
        filtered_word_counts = {word: count for word, count in word_counts.items() if count >= min_freq}
        common_terms = Counter(filtered_word_counts).most_common(top_n)
    else:
        common_terms = word_counts.most_common(top_n)
    
    return pd.DataFrame(common_terms, columns=['term', 'count'])

"""
@st.cache_data
def get_skill_cooccurrence_optimized(
    df: pd.DataFrame,
    text_column_to_scan: str,
    skill_keywords_list: list,
    top_n_cooc_pairs: int = 15,
    min_cooc_frequency: int = 2, # ä¸€ä¸ªæŠ€èƒ½å¯¹è‡³å°‘è¦å…±çŽ°è¿™ä¹ˆå¤šæ¬¡æ‰è¢«è€ƒè™‘
    min_skills_in_one_text: int = 2 # ä¸€æ¡æ–‡æœ¬ä¸­è‡³å°‘è¦è¯†åˆ«å‡ºè¿™ä¹ˆå¤šæŠ€èƒ½æ‰è¿›è¡Œç»„åˆ
) -> pd.DataFrame:
    
    è®¡ç®—æŠ€èƒ½åœ¨æŒ‡å®šæ–‡æœ¬åˆ—ä¸­çš„å…±çŽ°é¢‘çŽ‡ã€‚

    Args:
        df: åŒ…å«æ–‡æœ¬æ•°æ®çš„ DataFrameã€‚
        text_column_to_scan: DataFrame ä¸­å¾…æ‰«æçš„æ–‡æœ¬åˆ—åã€‚
        skill_keywords_list: ç”¨äºŽè¯†åˆ«æŠ€èƒ½çš„å…³é”®è¯åˆ—è¡¨ã€‚
        top_n_cooc_pairs: è¿”å›žçš„çƒ­é—¨å…±çŽ°æŠ€èƒ½å¯¹æ•°é‡ã€‚
        min_cooc_frequency: å…±çŽ°å¯¹çš„æœ€å°é¢‘çŽ‡é˜ˆå€¼ã€‚
        min_skills_in_one_text: å•ä¸ªæ–‡æœ¬ä¸­éœ€è¦è¯†åˆ«å‡ºçš„æœ€å°æŠ€èƒ½æ•°æ‰è¿›è¡Œå…±çŽ°è®¡ç®—ã€‚

    Returns:
        ä¸€ä¸ª DataFrameï¼ŒåŒ…å« 'skill_pair' å’Œ 'count' åˆ—ã€‚
    
    if df.empty or text_column_to_scan not in df.columns or not skill_keywords_list:
        # st.info("å…±çŽ°åˆ†æžï¼šè¾“å…¥æ•°æ®æˆ–æŠ€èƒ½å…³é”®è¯åˆ—è¡¨ä¸ºç©ºã€‚") # å¯ä»¥åœ¨è°ƒç”¨å¤„å¤„ç†
        return pd.DataFrame(columns=['skill_pair', 'count'])

    # 1. ä¸ºæŠ€èƒ½å…³é”®è¯æž„å»ºé«˜æ•ˆçš„åŒ¹é…æ¨¡å¼
    #    ç¡®ä¿ä¸Ž extract_skills_from_job_names ä¸­çš„æ­£åˆ™æž„å»ºé€»è¾‘ä¸€è‡´
    skill_patterns_map = {} # skill_original_case -> pattern
    keyword_lower_to_original_map = {kw.lower(): kw for kw in skill_keywords_list}

    for original_kw in skill_keywords_list:
        kw_lower = original_kw.lower()
        pattern_str = ""
        if re.search(r'[\u4e00-\u9fff]', original_kw): # ä¸­æ–‡æˆ–ä¸­è‹±æ··åˆ
            pattern_str = re.escape(original_kw)
        elif original_kw.isalnum(): # çº¯è‹±æ–‡/æ•°å­—
            pattern_str = r'\b' + re.escape(original_kw) + r'\b'
        else: # å«ç‰¹æ®Šç¬¦å·
            pattern_str = re.escape(original_kw)
        
        # ä½¿ç”¨åŽŸå§‹å¤§å°å†™çš„æŠ€èƒ½åä½œä¸º keyï¼Œæ–¹ä¾¿åŽç»­æŸ¥æ‰¾åŽŸå§‹å¤§å°å†™
        skill_patterns_map[original_kw] = pattern_str
        
    # æž„å»ºä¸€ä¸ªå¤§çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œä¸€æ¬¡æ€§åŒ¹é…æ‰€æœ‰æŠ€èƒ½
    # (?:pattern1|pattern2|...)
    # ä½¿ç”¨ re.IGNORECASEï¼Œæ‰€ä»¥ pattern æœ¬èº«ä¸éœ€è¦å¤„ç†å¤§å°å†™ï¼Œä½†åœ¨æ˜ å°„å›žåŽŸå§‹å¤§å°å†™æ—¶éœ€è¦
    # all_patterns_regex = r'(?:' + '|'.join(skill_patterns_map.values()) + r')'
    # ä¸Šé¢çš„æ–¹æ³•åœ¨æ˜ å°„å›žåŽŸå§‹å¤§å°å†™æ—¶å¤æ‚ï¼Œæ”¹ä¸ºé€ä¸ªæ¨¡å¼åŒ¹é…ï¼Œç„¶åŽæ˜ å°„

    cooccurrence_counts = Counter()
    processed_texts_count = 0
    texts_meeting_min_skills_criteria = 0

    for text_content in df[text_column_to_scan].dropna().astype(str):
        processed_texts_count += 1
        found_skills_in_current_text_original_case = set()

        # åœ¨å½“å‰æ–‡æœ¬ä¸­æŸ¥æ‰¾æ‰€æœ‰å®šä¹‰çš„æŠ€èƒ½å…³é”®è¯
        for original_skill_keyword, pattern in skill_patterns_map.items():
            # re.IGNORECASE ä½¿å¾—å…³é”®è¯åˆ—è¡¨ä¸­çš„å¤§å°å†™ä¸é‡è¦ï¼Œä½†æˆ‘ä»¬æƒ³ä¿ç•™åŽŸå§‹å¤§å°å†™ç”¨äºŽè¾“å‡º
            if re.search(pattern, text_content, re.IGNORECASE):
                found_skills_in_current_text_original_case.add(original_skill_keyword)
        
        if len(found_skills_in_current_text_original_case) >= min_skills_in_one_text:
            texts_meeting_min_skills_criteria += 1
            # å¯¹æ‰¾åˆ°çš„æŠ€èƒ½ï¼ˆåŽŸå§‹å¤§å°å†™ï¼‰è¿›è¡ŒæŽ’åºï¼Œä»¥ç¡®ä¿ ('A', 'B') å’Œ ('B', 'A') è¢«è§†ä¸ºç›¸åŒ
            # list(found_skills_in_current_text_original_case) ç¡®ä¿æ˜¯åˆ—è¡¨
            for combo in combinations(sorted(list(found_skills_in_current_text_original_case)), 2):
                cooccurrence_counts[combo] += 1

    # Debugging (å¯ä»¥åœ¨ Streamlit é¡µé¢ä¹‹å¤–æˆ–ä¸´æ—¶å–æ¶ˆæ³¨é‡Š)
    # print(f"DEBUG cooc_opt: Processed {processed_texts_count} texts from column '{text_column_to_scan}'.")
    # print(f"DEBUG cooc_opt: {texts_meeting_min_skills_criteria} texts had >= {min_skills_in_one_text} skills.")
    # print(f"DEBUG cooc_opt: Total unique co-occurring pairs found (before filtering): {len(cooccurrence_counts)}")
    # if cooccurrence_counts:
    #     print(f"DEBUG cooc_opt: Top 5 raw cooccurrence_counts: {dict(cooccurrence_counts.most_common(5))}")


    if not cooccurrence_counts:
        # st.info("å…±çŽ°åˆ†æžï¼šæœªæ‰¾åˆ°ä»»ä½•æŠ€èƒ½å…±çŽ°å¯¹ã€‚")
        return pd.DataFrame(columns=['skill_pair', 'count'])

    # è¿‡æ»¤æŽ‰é¢‘çŽ‡è¿‡ä½Žçš„å…±çŽ°å¯¹
    frequent_cooccurrences = {
        pair: count for pair, count in cooccurrence_counts.items() if count >= min_cooc_frequency
    }

    if not frequent_cooccurrences:
        # st.info(f"å…±çŽ°åˆ†æžï¼šæ‰€æœ‰å…±çŽ°å¯¹çš„é¢‘çŽ‡å‡ä½ŽäºŽè®¾å®šçš„æœ€å°é˜ˆå€¼ ({min_cooc_frequency})ã€‚")
        return pd.DataFrame(columns=['skill_pair', 'count'])

    # èŽ·å– top N
    # Counter(frequent_cooccurrences) ç¡®ä¿ most_common å¯ä»¥æ­£ç¡®å·¥ä½œ
    top_common_combos = Counter(frequent_cooccurrences).most_common(top_n_cooc_pairs)
    
    if not top_common_combos:
         # st.info(f"å…±çŽ°åˆ†æžï¼šè¿‡æ»¤åŽæ— æŠ€èƒ½å¯¹æ»¡è¶³Top N ({top_n_cooc_pairs}) æ¡ä»¶ã€‚")
        return pd.DataFrame(columns=['skill_pair', 'count'])

    combo_df = pd.DataFrame(top_common_combos, columns=['skill_pair_tuple', 'count'])
    combo_df['skill_pair'] = combo_df['skill_pair_tuple'].apply(lambda x: f"{x[0]} & {x[1]}")
    
    return combo_df[['skill_pair', 'count']].sort_values(by='count', ascending=False)
"""


@st.cache_data(ttl=3600) 
def load_json_data(file_path):
    actual_path_to_load = os.path.abspath(file_path)
    if not os.path.exists(actual_path_to_load):
        if streamlit_app_dir: st.error(f"Data file not found: {actual_path_to_load}")
        else: print(f"ERROR: Data file not found: {actual_path_to_load}")
        return pd.DataFrame()
    try:
        if actual_path_to_load.endswith('.jsonl'):
            records = []
            with open(actual_path_to_load, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        records.append(json.loads(line))
                    except json.JSONDecodeError as e_line:
                        if streamlit_app_dir: st.warning(f"Skipping invalid JSON line: {e_line} - Line: {line.strip()[:100]}...")
            df = pd.DataFrame(records)
        elif actual_path_to_load.endswith('.json'):
            with open(actual_path_to_load, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data) # Assumes list of dicts or dict of dicts suitable for DataFrame
        else:
            if streamlit_app_dir: st.error(f"Unsupported file format: {actual_path_to_load}")
            return pd.DataFrame()
        return df
    except Exception as e:
        if streamlit_app_dir: st.error(f"Error loading data from {actual_path_to_load}: {e}")
        else: print(f"ERROR loading data from {actual_path_to_load}: {e}")
        return pd.DataFrame()

# --- This is the main preprocess_jobs_data function we will keep and modify ---

PROVINCE_DIRECT_MAP = {
    "åŒ—äº¬å¸‚": "åŒ—äº¬", "å¤©æ´¥å¸‚": "å¤©æ´¥", "ä¸Šæµ·å¸‚": "ä¸Šæµ·", "é‡åº†å¸‚": "é‡åº†",
    "å†…è’™å¤è‡ªæ²»åŒº": "å†…è’™å¤", "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº": "å¹¿è¥¿", "å®å¤å›žæ—è‡ªæ²»åŒº": "å®å¤",
    "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº": "æ–°ç–†", "è¥¿è—è‡ªæ²»åŒº": "è¥¿è—",
    "é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº": "é¦™æ¸¯", "æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº": "æ¾³é—¨"
    #å°æ¹¾çœ -> å°æ¹¾ (if needed)
}

PROVINCE_SUFFIXES_TO_STRIP = ['çœ', 'å¸‚', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº'] # 'å¸‚' for direct municipalities
CITY_SUFFIXES_TO_STRIP = ['å¸‚', 'åœ°åŒº', 'è‡ªæ²»å·ž', 'ç›Ÿ', 'åŽ¿', 'åŒº'] # More comprehensive for city parts


@st.cache_data(ttl=3600)
def preprocess_jobs_data(df_jobs_raw):
    if df_jobs_raw.empty:
        return pd.DataFrame()

    df = df_jobs_raw.copy()

    PROVINCE_LEVEL_IDENTIFIERS = ['province', 'çœçº§', 'çœä»½', 'ç›´è¾–å¸‚', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº']
    CITY_LEVEL_IDENTIFIERS = ['city', 'å¸‚çº§', 'åŸŽå¸‚', 'åœ°åŒº', 'è‡ªæ²»å·ž', 'ç›Ÿ', 'æ–°åŒº']

    required_cols_defaults = {
        'job_id': None, 'job_name': 'æœªçŸ¥èŒä½', 'job_catory': 'æœªçŸ¥ç±»åˆ«', 'job_industry': 'æœªçŸ¥è¡Œä¸š',
        'high_month_pay': 0.0, 'low_month_pay': 0.0, 'publish_date': None, 'update_date': None,
        'company_name': 'æœªçŸ¥å…¬å¸', 'area_code_name': 'æœªçŸ¥åœ°åŒº', 'prinvce_code_nme': 'æœªçŸ¥çœä»½',
        'search_area_name': 'æœªçŸ¥åœ°åŒº', 'company_scale': 'æœªçŸ¥è§„æ¨¡', 'degree_name': 'å­¦åŽ†ä¸é™',
        'major_required': '', 'company_property': 'æœªçŸ¥æ€§è´¨', 'company_tags': '',
        'source_url': '#', 'head_count': 1, 'level': 'unknown', 'work_year': 'ç»éªŒä¸é™' # Added work_year
    }

    for col, default_val in required_cols_defaults.items():
        if col not in df.columns:
            df[col] = default_val
        else: # Fill NA for existing columns THEN convert type
            if col in ['major_required', 'company_tags', 'job_name', 'job_catory', 'job_industry',
                       'company_name', 'area_code_name', 'prinvce_code_nme', 'search_area_name',
                       'company_scale', 'degree_name', 'company_property', 'level', 'work_year', 'source_url']:
                df[col] = df[col].fillna(str(default_val)).astype(str)
            elif col in ['high_month_pay', 'low_month_pay', 'head_count']:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(default_val if isinstance(default_val, (int, float)) else 0)
            elif col in ['publish_date', 'update_date']:
                 df[col] = pd.to_numeric(df[col], errors='coerce')
    
    df['avg_month_pay'] = df.apply(
        lambda row: (row['low_month_pay'] + row['high_month_pay']) / 2 if row['low_month_pay'] > 0 and row['high_month_pay'] > 0 else \
                    row['high_month_pay'] if row['high_month_pay'] > 0 else \
                    row['low_month_pay'] if row['low_month_pay'] > 0 else 0.0,
        axis=1
    )
    df['avg_month_pay'] = pd.to_numeric(df['avg_month_pay'], errors='coerce').fillna(0.0)

    for date_col in ['publish_date', 'update_date']:
        if date_col in df.columns:
            df[f'{date_col}_dt'] = pd.to_datetime(df[date_col], unit='ms', errors='coerce', utc=True)

    # --- Geographical Name Cleaning ---
    df['province_clean'] = df['prinvce_code_nme'].astype(str)
    for full, short in PROVINCE_DIRECT_MAP.items():
        df['province_clean'] = df['province_clean'].str.replace(full, short, regex=False)
    for suffix in PROVINCE_SUFFIXES_TO_STRIP:
        df['province_clean'] = df['province_clean'].str.replace(suffix, '', regex=False)
    df['province_clean'] = df['province_clean'].str.strip()
    df.loc[df['province_clean'] == '', 'province_clean'] = 'æœªçŸ¥çœä»½'

    df['city_clean'] = 'æœªçŸ¥åŸŽå¸‚' # Initialize column
    for index, row in df.iterrows():
        level_val = str(row['level']).strip().lower()
        original_search_area = str(row['search_area_name']).strip()
        original_prinvce_name = str(row['prinvce_code_nme']).strip()
        cleaned_province_name = str(row['province_clean']).strip()
        current_city_val = "æœªçŸ¥åŸŽå¸‚"

        if any(prov_id in level_val for prov_id in PROVINCE_LEVEL_IDENTIFIERS):
            current_city_val = cleaned_province_name
        elif any(city_id in level_val for city_id in CITY_LEVEL_IDENTIFIERS):
            city_candidate = original_search_area
            if original_search_area.startswith(original_prinvce_name) and len(original_search_area) > len(original_prinvce_name):
                city_candidate = original_search_area[len(original_prinvce_name):].strip()
            
            if city_candidate == original_prinvce_name or not city_candidate:
                current_city_val = cleaned_province_name
            else:
                temp_city_name = city_candidate
                for city_suffix in CITY_SUFFIXES_TO_STRIP:
                    temp_city_name = temp_city_name.replace(city_suffix, '')
                current_city_val = temp_city_name.strip()
                if not current_city_val: # If stripping made it empty
                    current_city_val = city_candidate # Revert to pre-stripped version
                    if not current_city_val: current_city_val = cleaned_province_name # Ultimate fallback
        else: # Fallback for 'unknown' level or other levels
            if original_search_area == original_prinvce_name or original_search_area == cleaned_province_name:
                current_city_val = cleaned_province_name
            else:
                city_candidate = original_search_area
                # Attempt to remove province prefix from search_area if present
                if original_search_area.startswith(original_prinvce_name) and len(original_search_area) > len(original_prinvce_name):
                    city_candidate = original_search_area[len(original_prinvce_name):].strip()
                
                temp_city_name = city_candidate
                for city_suffix in CITY_SUFFIXES_TO_STRIP:
                    temp_city_name = temp_city_name.replace(city_suffix, '')
                current_city_val = temp_city_name.strip()
                if not current_city_val: current_city_val = cleaned_province_name # Fallback
        
        df.loc[index, 'city_clean'] = current_city_val if current_city_val else "æœªçŸ¥åŸŽå¸‚"
    
    df.loc[df['city_clean'].isin(['', 'æœªçŸ¥åœ°åŒº', 'æœªçŸ¥åŸŽå¸‚']) & (df['province_clean'] != 'æœªçŸ¥çœä»½'), 'city_clean'] = df['province_clean']
    df.loc[df['city_clean'] == '', 'city_clean'] = 'æœªçŸ¥åŸŽå¸‚'
    # --- End Geographical Name Cleaning ---

    # --- Company Scale ---
    # ä½¿ç”¨æ‚¨æä¾›çš„ scale_order
    TARGET_SCALE_CATEGORIES_ORDERED = ["1-49äºº", "50-99äºº", "100-499äºº", "500-999äºº", "1000-9999äºº", "10000+äºº", "æœªçŸ¥"]
    
    # å°è¯•ä»Ž target_options.json åŠ è½½ scalesï¼Œå¦‚æžœå­˜åœ¨ä¸”æœ‰æ•ˆï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å®ƒ
    # å¦åˆ™å›žé€€åˆ°æ‚¨æŒ‡å®šçš„ TARGET_SCALE_CATEGORIES_ORDERED
    custom_scale_order_from_options = []
    if os.path.exists(DEFAULT_OPTIONS_FILE_PATH):
        try:
            with open(DEFAULT_OPTIONS_FILE_PATH, 'r', encoding='utf-8') as f:
                target_options_content = json.load(f) # Renamed to avoid conflict
            if 'scales' in target_options_content and isinstance(target_options_content['scales'], list):
                loaded_scales = []
                for item in target_options_content['scales']:
                    if isinstance(item, dict) and 'name' in item and isinstance(item['name'], str):
                        loaded_scales.append(item['name'])
                    elif isinstance(item, str): # å¦‚æžœåˆ—è¡¨é‡Œç›´æŽ¥æ˜¯å­—ç¬¦ä¸²
                        loaded_scales.append(item)
                
                if loaded_scales: # å¦‚æžœæˆåŠŸä»Žæ–‡ä»¶åŠ è½½äº†æœ‰æ•ˆçš„è§„æ¨¡åˆ—è¡¨
                    if "æœªçŸ¥" not in loaded_scales: # ç¡®ä¿ "æœªçŸ¥" ç±»åˆ«å­˜åœ¨
                        loaded_scales.append("æœªçŸ¥")
                    TARGET_SCALE_CATEGORIES_ORDERED = loaded_scales # è¦†ç›–é»˜è®¤å€¼
                    print(f"Info: Company scale categories loaded from '{os.path.basename(DEFAULT_OPTIONS_FILE_PATH)}': {TARGET_SCALE_CATEGORIES_ORDERED}")
                else:
                    print(f"Info: 'scales' in '{os.path.basename(DEFAULT_OPTIONS_FILE_PATH)}' was empty or invalid. Using predefined scale order.")
            else:
                print(f"Info: 'scales' key not found or not a list in '{os.path.basename(DEFAULT_OPTIONS_FILE_PATH)}'. Using predefined scale order.")
        except Exception as e_scale_load:
            print(f"Warning: Error loading company scales from '{os.path.basename(DEFAULT_OPTIONS_FILE_PATH)}': {e_scale_load}. Using predefined scale order.")
            # ç¡®ä¿ TARGET_SCALE_CATEGORIES_ORDERED ä»ç„¶æ˜¯æ‚¨æä¾›çš„åˆ—è¡¨
            if "æœªçŸ¥" not in TARGET_SCALE_CATEGORIES_ORDERED: # å†æ¬¡æ£€æŸ¥ï¼Œä»¥é˜²ä¸‡ä¸€
                TARGET_SCALE_CATEGORIES_ORDERED.append("æœªçŸ¥")


    def clean_and_map_scale(raw_scale_str):
        if pd.isna(raw_scale_str): return "æœªçŸ¥"
        s = str(raw_scale_str).strip().lower()
        if not s or s in ['nan', 'none', 'null', 'ä¿å¯†', 'ä¸è¯¦', 'æœªçŸ¥', 'ä¸æ˜Žç¡®', 'ä¸é™å…¬å¸è§„æ¨¡', '-1', '0']: return "æœªçŸ¥"

        # ä¼˜å…ˆç²¾ç¡®åŒ¹é… target_options.json ä¸­å®šä¹‰çš„ç±»åˆ« (å¦‚æžœå·²åŠ è½½)
        # æˆ–æ‚¨æä¾›çš„ TARGET_SCALE_CATEGORIES_ORDERED
        for target_scale in TARGET_SCALE_CATEGORIES_ORDERED:
            if target_scale.lower() == s:
                return target_scale # è¿”å›žåŽŸå§‹å¤§å°å†™çš„ç›®æ ‡ç±»åˆ«

        # åŸºäºŽå…³é”®è¯çš„æ¨¡ç³ŠåŒ¹é…å’ŒèŒƒå›´æ˜ å°„ (æ˜ å°„åˆ°æ‚¨æä¾›çš„ scale_order)
        # "1-49äºº"
        if any(x in s for x in ["å°‘äºŽ15", "1-15", "0-20", "20äººä»¥ä¸‹", "1-49", "å°‘äºŽ50äºº"]): return "1-49äºº" # åŒ…å«åŽŸ "å°‘äºŽ50äºº"
        # "50-99äºº"
        if any(x in s for x in ["15-50", "20-99", "50-99"]): return "50-99äºº"
        # "100-499äºº"
        if any(x in s for x in ["50-150", "100-499", "150-499"]): return "100-499äºº"
        # "500-999äºº"
        if any(x in s for x in ["150-500", "500-999", "500äººä»¥ä¸‹", "åƒäººä»¥ä¸‹"]): return "500-999äºº" # 500äººä»¥ä¸‹å¯èƒ½éœ€è¦æ›´ç»†è‡´åˆ¤æ–­ï¼Œä½†è¿™é‡Œå½’å…¥
        # "1000-9999äºº"
        if any(x in s for x in ["500-2000", "1000-4999", "1000-9999", "1000-2000", "2000-5000", "5000-9999", "åƒäººè§„æ¨¡", "å‡ åƒäºº"]): return "1000-9999äºº"
        # "10000+äºº"
        if any(x in s for x in ["2000äººä»¥ä¸Š", "5000äººä»¥ä¸Š", "10000äººä»¥ä¸Š", "10000+", "ä¸‡äººä»¥ä¸Š", "ä¸Šå¸‚å…¬å¸", "å¤§åž‹ä¼ä¸š"]): return "10000+äºº"
        
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ•°å­—èŒƒå›´
        m = re.match(r'(\d+)[äºº\s]*-?[è‡³åˆ°]?\s*(\d+)\s*äºº?', s) # åŒ¹é… "æ•°å­—-æ•°å­—äºº", "æ•°å­—äºº-æ•°å­—äºº", "æ•°å­— è‡³/åˆ° æ•°å­—äºº"
        if m:
            low, high = int(m.group(1)), int(m.group(2))
            if high < 50: return "1-49äºº"
            if low >=1 and high <= 49: return "1-49äºº"
            if low >=50 and high <= 99: return "50-99äºº"
            if low >=100 and high <= 499: return "100-499äºº"
            if low >=500 and high <= 999: return "500-999äºº"
            if low >=1000 and high <= 9999: return "1000-9999äºº"
            if low >= 10000: return "10000+äºº"
            # æ›´å®½æ¾çš„èŒƒå›´åˆ¤æ–­
            if high <= 49: return "1-49äºº"
            if high <= 99: return "50-99äºº"
            if high <= 499: return "100-499äºº"
            if high <= 999: return "500-999äºº"
            if high <= 9999: return "1000-9999äºº"
            return "10000+äºº" # é»˜è®¤æ›´å¤§çš„

        m_above = re.match(r'(\d+)\s*äºº?\s*(ä»¥ä¸Š|\+)', s) # åŒ¹é… "æ•°å­—äººä»¥ä¸Š", "æ•°å­—+"
        if m_above:
            val = int(m_above.group(1))
            if val >= 10000: return "10000+äºº"
            if val >= 1000: return "1000-9999äºº" # å¦‚æžœæ˜¯1000äººä»¥ä¸Šï¼Œå¤§éƒ¨åˆ†æƒ…å†µæ˜¯è¿™ä¸ªæ¡£
            if val >= 500: return "500-999äºº"
            if val >= 100: return "100-499äºº"
            if val >= 50: return "50-99äºº"
            return "1-49äºº" # ä¾‹å¦‚ 20äººä»¥ä¸Š

        m_below = re.match(r'å°‘äºŽ\s*(\d+)\s*äºº', s) # åŒ¹é… "å°‘äºŽ æ•°å­— äºº"
        if m_below:
            val = int(m_below.group(1))
            if val <= 50 : return "1-49äºº" # å°‘äºŽ50äºº å½’å…¥ 1-49äºº
            if val <= 100: return "50-99äºº"
            # ... å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤š
            
        return "æœªçŸ¥"

    df['company_scale_cleaned_mapped'] = df['company_scale'].apply(clean_and_map_scale)
    
    # ç¡®ä¿æ‰€æœ‰æ˜ å°„åŽçš„å€¼éƒ½åœ¨ TARGET_SCALE_CATEGORIES_ORDERED ä¸­ï¼Œå¦‚æžœä¸åœ¨åˆ™è®¾ä¸º"æœªçŸ¥"
    # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå› ä¸º clean_and_map_scale å¯èƒ½äº§ç”Ÿä¸åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„ä¸­é—´å€¼
    unique_cleaned_scales = df['company_scale_cleaned_mapped'].unique()
    for scale_val in unique_cleaned_scales:
        if scale_val not in TARGET_SCALE_CATEGORIES_ORDERED:
            df.loc[df['company_scale_cleaned_mapped'] == scale_val, 'company_scale_cleaned_mapped'] = "æœªçŸ¥"
            
    df['company_scale_cat'] = pd.Categorical(
        df['company_scale_cleaned_mapped'], 
        categories=TARGET_SCALE_CATEGORIES_ORDERED, 
        ordered=True
    )
    # å†æ¬¡å¡«å……å¯èƒ½å› å¼ºåˆ¶è½¬æ¢ä¸ºCategoricaläº§ç”Ÿçš„NaNï¼ˆå¦‚æžœå€¼ä¸åœ¨categoriesä¸­ï¼‰
    df['company_scale_cat'] = df['company_scale_cat'].fillna("æœªçŸ¥")


    # --- Degree Name ---
    TARGET_DEGREE_CATEGORIES_ORDERED = ['å­¦åŽ†ä¸é™', 'å…¶ä»–', 'åˆä¸­åŠä»¥ä¸‹', 'ä¸­ä¸“', 'ä¸­æŠ€', 'ä¸­ä¸“/ä¸­æŠ€', 'é«˜ä¸­', 'å¤§ä¸“', 'æœ¬ç§‘', 'ç¡•å£«', 'åšå£«', 'åšå£«åŽ']
    custom_degree_order_from_options = []
    if os.path.exists(DEFAULT_OPTIONS_FILE_PATH):
        try:
            with open(DEFAULT_OPTIONS_FILE_PATH, 'r', encoding='utf-8') as f: target_options_content_degree = json.load(f)
            if 'degrees' in target_options_content_degree and isinstance(target_options_content_degree['degrees'], list):
                loaded_degrees = []
                for item in target_options_content_degree['degrees']:
                    if isinstance(item, dict) and 'name' in item and isinstance(item['name'], str): loaded_degrees.append(item['name'])
                    elif isinstance(item, str): loaded_degrees.append(item)
                if loaded_degrees:
                    if 'å­¦åŽ†ä¸é™' not in loaded_degrees and 'ä¸é™' not in loaded_degrees : loaded_degrees.insert(0, 'å­¦åŽ†ä¸é™')
                    if 'å…¶ä»–' not in loaded_degrees: loaded_degrees.insert(1, 'å…¶ä»–')
                    TARGET_DEGREE_CATEGORIES_ORDERED = loaded_degrees
        except Exception: pass
    
    df['degree_name'] = df['degree_name'].fillna('å­¦åŽ†ä¸é™').astype(str)
    df.loc[df['degree_name'].str.lower().isin(['ä¸é™', '', 'nan', 'none', 'null', '-1']), 'degree_name'] = 'å­¦åŽ†ä¸é™'
    def map_to_standard_degree(raw_degree_str):
        s_raw = str(raw_degree_str)
        s_lower = s_raw.lower()
        if s_lower in ['å­¦åŽ†ä¸é™', 'ä¸é™', '', 'nan', 'none', 'null', '-1', '0']: return 'å­¦åŽ†ä¸é™'
        for target_cat in TARGET_DEGREE_CATEGORIES_ORDERED: # ç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™è¾“å…¥ï¼Œä½†ç”¨ç›®æ ‡ç±»åˆ«çš„å¤§å°å†™ï¼‰
            if target_cat.lower() == s_lower: return target_cat
        # æ¨¡ç³ŠåŒ¹é…
        if 'åˆä¸­' in s_lower: return 'åˆä¸­åŠä»¥ä¸‹'
        if 'ä¸­ä¸“' in s_lower or 'ä¸­æŠ€' in s_lower: return 'ä¸­ä¸“/ä¸­æŠ€' # å‡è®¾ç›®æ ‡åˆ—è¡¨æœ‰ "ä¸­ä¸“/ä¸­æŠ€"
        if 'é«˜ä¸­' in s_lower: return 'é«˜ä¸­'
        if 'å¤§ä¸“' in s_lower or 'ä¸“ç§‘' in s_lower: return 'å¤§ä¸“'
        if 'æœ¬ç§‘' in s_lower or 'å­¦å£«' in s_lower: return 'æœ¬ç§‘'
        if 'ç¡•å£«' in s_lower or ('ç ”ç©¶ç”Ÿ' in s_raw and 'åšå£«' not in s_raw): return 'ç¡•å£«'
        if 'åšå£«åŽ' in s_lower: return 'åšå£«åŽ' # åšå£«åŽä¼˜å…ˆäºŽåšå£«
        if 'åšå£«' in s_lower: return 'åšå£«'
        return 'å…¶ä»–'
    df['degree_name_mapped'] = df['degree_name'].apply(map_to_standard_degree)
    unique_mapped_degrees = df['degree_name_mapped'].unique()
    for degree_val in unique_mapped_degrees:
        if degree_val not in TARGET_DEGREE_CATEGORIES_ORDERED:
            df.loc[df['degree_name_mapped'] == degree_val, 'degree_name_mapped'] = 'å…¶ä»–'
            if 'å…¶ä»–' not in TARGET_DEGREE_CATEGORIES_ORDERED: TARGET_DEGREE_CATEGORIES_ORDERED.append('å…¶ä»–') # ç¡®ä¿'å…¶ä»–'åœ¨åˆ—è¡¨ä¸­
    df['degree_name_cat'] = pd.Categorical(df['degree_name_mapped'], categories=TARGET_DEGREE_CATEGORIES_ORDERED, ordered=True)
    df['degree_name_cat'] = df['degree_name_cat'].fillna('å…¶ä»–')


    # --- Work Year ---
    df['work_year_numeric'] = pd.to_numeric(df['work_year'], errors='coerce') 
    df.loc[df['work_year'].isin(['ä¸é™', 'ç»éªŒä¸é™', 'æ— ç»éªŒ', 'åº”å±Šæ¯•ä¸šç”Ÿ', 'åº”å±Šç”Ÿ', 'åœ¨æ ¡ç”Ÿ', '-1', '0', 'æ— éœ€ç»éªŒ']), 'work_year_numeric'] = 0
    def parse_work_year_text(text):
        if pd.isna(text) or isinstance(text, (int, float)): return text
        text_str = str(text).strip()
        if not text_str or text_str.lower() in ['ä¸é™', 'ç»éªŒä¸é™', 'æ— ç»éªŒ', 'åº”å±Šæ¯•ä¸šç”Ÿ', 'åº”å±Šç”Ÿ', 'åœ¨æ ¡ç”Ÿ', '-1', '0', 'æ— éœ€ç»éªŒ']: return 0
        if "å¹´ä»¥å†…" in text_str or "1å¹´ä»¥ä¸‹" in text_str or "ä¸€å¹´ä»¥å†…" in text_str: return 0.5
        if "å¹´ä»¥ä¸Š" in text_str:
            match = re.search(r'(\d+)', text_str)
            return int(match.group(1)) if match else np.nan
        match_range = re.search(r'(\d+)[-\sè‡³åˆ°]*(\d+)å¹´', text_str)
        if match_range:
            return (int(match_range.group(1)) + int(match_range.group(2))) / 2
        match_single = re.search(r'(\d+)å¹´', text_str)
        if match_single: return int(match_single.group(1))
        return np.nan
    
    # ä¼˜å…ˆä½¿ç”¨ work_year_numeric å¦‚æžœå®ƒå·²ç»æ˜¯æœ‰æ•ˆçš„æ•°å­—ï¼Œå¦åˆ™å°è¯•ä»Žæ–‡æœ¬è§£æž
    df['work_year_numeric'] = df.apply(
        lambda row: row['work_year_numeric'] if pd.notna(row['work_year_numeric']) else parse_work_year_text(row['work_year']),
        axis=1
    )
    df['work_year_numeric'] = pd.to_numeric(df['work_year_numeric'], errors='coerce').fillna(-1) # -1 ä»£è¡¨"ç»éªŒä¸é™"è¿›å…¥ç‰¹å®šbin

    bins = [-np.inf, 0, 1, 3, 5, 10, np.inf] 
    labels = ['ç»éªŒä¸é™', '1å¹´ä»¥å†…', '1-3å¹´', '3-5å¹´', '5-10å¹´', '10å¹´ä»¥ä¸Š']
    # å¦‚æžœè¦å®Œå…¨åŒ¹é…æ‚¨ä¹‹å‰çš„ bins/labels (å³ -1 -> 'ç»éªŒä¸é™', 0 -> '1å¹´ä»¥å†…')
    bins = [-np.inf, -0.5, 0.5, 2.5, 4.5, 7.5, np.inf] # è°ƒæ•´ bins ä½¿0è½åœ¨ '1å¹´ä»¥å†…'
    labels = ['ç»éªŒä¸é™', '1å¹´ä»¥å†…', '1-3å¹´', '3-5å¹´', '5-10å¹´', '10å¹´ä»¥ä¸Š']
    # current bins: -1 (ç»éªŒä¸é™), 0 (1å¹´ä»¥å†…), 1,2 (1-3å¹´), 3,4 (3-5å¹´), 5-9 (5-10å¹´), 10+ (10å¹´ä»¥ä¸Š)
    # The current bins=[-np.inf, 0, 1, 3, 5, 10, np.inf] and labels mean:
    #   val < 0  -> 'ç»éªŒä¸é™' (includes our -1 for "ç»éªŒä¸é™")
    #   0 <= val < 1 -> '1å¹´ä»¥å†…'
    #   1 <= val < 3 -> '1-3å¹´'
    #   3 <= val < 5 -> '3-5å¹´'
    #   5 <= val < 10 -> '5-10å¹´'
    #   val >= 10 -> '10å¹´ä»¥ä¸Š'
    # This seems reasonable.
    df['work_year_cat'] = pd.cut(df['work_year_numeric'], bins=bins, labels=labels, right=False, include_lowest=True)
    df['work_year_cat'] = df['work_year_cat'].cat.reorder_categories(labels, ordered=True).fillna('ç»éªŒä¸é™')


    # --- Major, Tags, Skills Text ---
    common_none_terms = ['none', 'null', 'æ— ', '', 'nan', '-1']
    standard_majors_list = _load_standard_majors() # Cached
    df['major_required_cleaned'] = df['major_required'].astype(str).str.lower().str.strip()
    df['major_required_cleaned'].replace(common_none_terms + ['ä¸é™', 'æœªçŸ¥', 'ä¸“ä¸šä¸é™', 'ç›¸å…³ä¸“ä¸š', 'ä¸è¦æ±‚', 'å…¶ä»–'], 'æœªçŸ¥', inplace=True)
    if standard_majors_list:
        df['processed_major'] = df['major_required_cleaned'].apply(
            lambda x: _map_to_standard_major(x, standard_majors_list) if x != 'æœªçŸ¥' else 'æœªçŸ¥'
        )
    else:
        df['processed_major'] = df['major_required_cleaned'].apply(lambda x: 'å…¶ä»–ä¸“ä¸š' if x and x != 'æœªçŸ¥' else 'æœªçŸ¥')

    def split_raw_majors(text):
        if not text or text.lower().strip() in common_none_terms + ['ä¸é™', 'æœªçŸ¥', 'ä¸“ä¸šä¸é™', 'ç›¸å…³ä¸“ä¸š', 'ä¸è¦æ±‚', 'å…¶ä»–']: return tuple()
        text_cleaned = text.replace("æˆ–ç›¸å…³ä¸“ä¸š", " ç›¸å…³ä¸“ä¸š").replace("åŠç›¸å…³ä¸“ä¸š", " ç›¸å…³ä¸“ä¸š")
        delimiters = r"[\s,ï¼Œã€;/()ï¼ˆï¼‰]+" # åŒ…æ‹¬æ‹¬å·
        raw_majors = [m.strip() for m in re.split(delimiters, text_cleaned) if m.strip() and m.strip().lower() not in common_none_terms and len(m.strip()) > 1 and "ç›¸å…³ä¸“ä¸š" not in m and "ä¸“ä¸š" not in m] # é¿å…å•ç‹¬çš„"ä¸“ä¸š"
        return tuple(m for m in raw_majors if m)
    df['majors_list'] = df['major_required'].apply(split_raw_majors)

    df['company_tags'] = df['company_tags'].astype(str).fillna('')
    df['tags_list'] = df['company_tags'].apply(
        lambda x: tuple(t.strip() for t in x.split('ï¼Œ') if t.strip() and t.strip().lower() not in common_none_terms) 
        if x.strip() and x.strip().lower() not in common_none_terms else tuple()
    )

    df['job_name_and_major_text'] = df['job_name'].astype(str).fillna('') + " " + df['major_required'].astype(str).fillna('')
    if 'job_description' in df.columns:
        df['job_name_and_major_text'] += " " + df['job_description'].astype(str).fillna('')
    # åœ¨è¿™é‡Œè°ƒç”¨ extract_skills_from_text_series
    df['extracted_skills_list'] = extract_skills_from_text_series(df['job_name_and_major_text'], SKILL_REGEX_PATTERNS)


    # --- Company Property ---
    if 'company_property' in df.columns and os.path.exists(DEFAULT_OPTIONS_FILE_PATH):
        try:
            with open(DEFAULT_OPTIONS_FILE_PATH, 'r', encoding='utf-8') as f: target_options_content_prop = json.load(f)
            if 'corpProps' in target_options_content_prop and isinstance(target_options_content_prop['corpProps'], list):
                prop_map = {str(item['code']): item['name'] for item in target_options_content_prop['corpProps'] if isinstance(item, dict) and 'code' in item and 'name' in item}
                # å…ˆæ˜ å°„ï¼Œå¯¹äºŽæ— æ³•æ˜ å°„çš„ï¼Œä¿ç•™åŽŸå€¼ï¼ŒåŽç»­å†ç»Ÿä¸€å¤„ç†
                df['company_property'] = df['company_property'].astype(str).map(prop_map).fillna(df['company_property'])
                valid_prop_names = list(prop_map.values()) + ['æœªçŸ¥æ€§è´¨'] # åŒ…æ‹¬æˆ‘ä»¬æœŸæœ›çš„æœªçŸ¥å€¼
                # å¯¹äºŽé‚£äº›ä¸æ˜¯æœ‰æ•ˆåç§°ï¼ˆä¹Ÿä¸ä¸ºç©º/NaNï¼‰çš„å€¼ï¼Œè®¾ä¸º "æœªçŸ¥æ€§è´¨"
                df.loc[~df['company_property'].isin(valid_prop_names) & df['company_property'].notna() & (df['company_property'] != ''), 'company_property'] = 'æœªçŸ¥æ€§è´¨'
                df['company_property'] = df['company_property'].fillna('æœªçŸ¥æ€§è´¨') # å¡«å……æ‰€æœ‰å‰©ä¸‹çš„NaN
        except Exception: # å¦‚æžœåŠ è½½æˆ–æ˜ å°„å¤±è´¥
            df['company_property'] = df['company_property'].fillna('æœªçŸ¥æ€§è´¨').astype(str)
            df.loc[df['company_property'].astype(str).str.match(r'^\d+$|^-$'), 'company_property'] = 'æœªçŸ¥æ€§è´¨'
    elif 'company_property' in df.columns: # å¦‚æžœæ–‡ä»¶ä¸å­˜åœ¨ä½†åˆ—å­˜åœ¨
        df['company_property'] = df['company_property'].fillna('æœªçŸ¥æ€§è´¨').astype(str)
        df.loc[df['company_property'].astype(str).str.match(r'^\d+$|^-$'), 'company_property'] = 'æœªçŸ¥æ€§è´¨' # æ¸…ç†æ•°å­—ä»£ç 
    else: # å¦‚æžœåˆ—ä¹Ÿä¸å­˜åœ¨
        df['company_property'] = 'æœªçŸ¥æ€§è´¨'
    df.loc[df['company_property'].isin(['', '-1', '0', 'ä¸è¯¦']), 'company_property'] = 'æœªçŸ¥æ€§è´¨' # æœ€ç»ˆæ•èŽ·


    # Final check for consistency in "æœªçŸ¥" type values across key categorical columns
    unknown_synonyms_map = {
        'job_catory': 'æœªçŸ¥ç±»åˆ«',
        'job_industry': 'æœªçŸ¥è¡Œä¸š',
        'company_property': 'æœªçŸ¥æ€§è´¨' # å·²ç»å¤„ç†è¿‡äº†ï¼Œä½†å¯ä»¥å†æ¬¡ç¡®ä¿
    }
    raw_unknowns_to_catch = ['æœªçŸ¥', 'ä¸è¯¦', 'ä¿å¯†', 'ä¸æ˜Žç¡®', 'å…¶ä»–', '', '-1', '0', 'null', 'none', 'nan'] # æ›´å…¨é¢çš„åˆ—è¡¨

    for col_name, standard_unknown_val in unknown_synonyms_map.items():
        if col_name in df.columns:
            df[col_name] = df[col_name].astype(str).str.strip()
            for syn in raw_unknowns_to_catch:
                 # ä½¿ç”¨ .str.lower() æ¥åŒ¹é…ï¼Œä½†æ›¿æ¢ä¸º standard_unknown_val (ä¿ç•™å…¶å¤§å°å†™)
                df.loc[df[col_name].str.lower() == syn.lower(), col_name] = standard_unknown_val
            df.loc[df[col_name].str.strip() == '', col_name] = standard_unknown_val # ç¡®ä¿ç©ºå­—ç¬¦ä¸²ä¹Ÿè¢«å¤„ç†
            df[col_name] = df[col_name].fillna(standard_unknown_val) # ä»¥é˜²ä¸‡ä¸€çš„ fillna

    return df

# --- Analysis functions ---
@st.cache_data
def get_top_n_counts(df, column_name, top_n=10):
    if df.empty or column_name not in df.columns:
        return pd.DataFrame(columns=[column_name, 'count'])
    
    # Ensure column is suitable for value_counts (e.g., not all NaN)
    if df[column_name].dropna().empty:
        return pd.DataFrame(columns=[column_name, 'count'])
        
    counts = df[column_name].value_counts().nlargest(top_n).reset_index()
    # Ensure correct column names after reset_index
    counts.columns = [column_name, 'count'] if len(counts.columns) == 2 else ['item', 'count'] # Fallback
    if counts.columns[0] != column_name and 'item' == counts.columns[0]: # Rename if needed
        counts.rename(columns={'item': column_name}, inplace=True)

    return counts

@st.cache_data
def get_average_salary(df, group_by_col):
    if df.empty or group_by_col not in df.columns or 'avg_month_pay' not in df.columns:
        return pd.DataFrame(columns=[group_by_col, 'average_salary', 'median_salary', 'job_count'])
    
    valid_salary_df = df[(df['avg_month_pay'] > 0) & df[group_by_col].notna()] # Also filter out NaN group_by keys
    if valid_salary_df.empty:
        return pd.DataFrame(columns=[group_by_col, 'average_salary', 'median_salary', 'job_count'])
    
    # Using observed=True is generally safer for categorical data if categories might not all be present
    result = valid_salary_df.groupby(group_by_col, observed=True).agg( 
        average_salary=('avg_month_pay', 'mean'),
        median_salary=('avg_month_pay', 'median'),
        job_count=('job_id', 'count') # Assuming job_id is unique identifier
    ).reset_index()

    result['average_salary'] = result['average_salary'].round(1)
    result['median_salary'] = result['median_salary'].round(1)
    return result.sort_values(by='average_salary', ascending=False)

@st.cache_data
def get_avg_salary_by_city(df_jobs): 
    return get_average_salary(df_jobs, 'city_clean')

@st.cache_data
def get_avg_salary_by_province(df_jobs):
    return get_average_salary(df_jobs, 'province_clean')

@st.cache_data
def get_avg_salary_by_category(df_jobs):
    return get_average_salary(df_jobs, 'job_catory')

@st.cache_data
def get_avg_salary_by_processed_major(df_jobs): # For standardized majors
    return get_average_salary(df_jobs, 'processed_major')


@st.cache_data
def get_word_counts_from_list_column(df, list_column_name, top_n=20, exclude_items=None): # Name was changed slightly in original, kept this more specific one
    if df.empty or list_column_name not in df.columns:
        return pd.DataFrame(columns=['item', 'count'])
    
    default_exclude = {'none', 'null', 'æ— ', '', 'æœªçŸ¥', 'ä¸é™', 'ç›¸å…³ä¸“ä¸š'} # Use a set for faster lookups
    current_exclude_items = default_exclude.copy()
    if exclude_items:
        current_exclude_items.update(str(item).lower().strip() for item in exclude_items)

    all_items_from_col = []
    # This column should contain iterables (lists/tuples of strings)
    for item_iterable in df[list_column_name].dropna():
        if isinstance(item_iterable, (list, tuple)): 
            for item in item_iterable:
                cleaned_item = str(item).strip() 
                # Add more filtering: e.g. min length, specific unwanted terms
                if cleaned_item and cleaned_item.lower() not in current_exclude_items and len(cleaned_item) > 1:
                    all_items_from_col.append(cleaned_item)
        # else: # If a row has a single string instead of a list/tuple
            # cleaned_item = str(item_iterable).strip()
            # if cleaned_item and cleaned_item.lower() not in current_exclude_items and len(cleaned_item) > 1:
            #     all_items_from_col.append(cleaned_item)
    
    if not all_items_from_col:
        return pd.DataFrame(columns=['item', 'count'])
        
    counts = Counter(all_items_from_col)
    top_items = counts.most_common(top_n)
    return pd.DataFrame(top_items, columns=['item', 'count'])


@st.cache_data
def get_time_series_data(df, time_col='publish_date_dt', freq='ME', value_col=None): # Default to Month End
    if df.empty or time_col not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[time_col]):
        return pd.Series(dtype='float64' if value_col else 'int64')

    df_time = df.dropna(subset=[time_col])
    if df_time.empty:
        return pd.Series(dtype='float64' if value_col else 'int64')

    # Ensure time_col is UTC before resampling, or handle naive datetimes appropriately
    if df_time[time_col].dt.tz is None: # If naive
        # Assuming naive datetimes are effectively UTC or should be treated as such for resampling
        df_time_indexed = df_time.set_index(df_time[time_col].dt.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT'))
    else: # Already timezone-aware
        df_time_indexed = df_time.set_index(df_time[time_col].dt.tz_convert('UTC'))


    if value_col and value_col in df_time_indexed.columns:
        series = df_time_indexed[value_col].resample(freq).mean()
    else: # Default to counting job_id
        series = df_time_indexed['job_id'].resample(freq).count() if 'job_id' in df_time_indexed else df_time_indexed.iloc[:,0].resample(freq).count()

    
    series = series.fillna(0) 

    if not series.empty: # Trim leading/trailing zeros
        try:
            first_valid_index = series.ne(0).idxmax() 
            last_valid_index = series.ne(0)[::-1].idxmax() 
            if pd.notna(first_valid_index) and pd.notna(last_valid_index) and first_valid_index <= last_valid_index:
                series = series[first_valid_index:last_valid_index]
            else: # All zeros or invalid range
                return pd.Series(dtype=series.dtype)
        except ValueError: # Handles cases like all zeros
            return pd.Series(dtype=series.dtype)
            
    return series

@st.cache_data
def extract_skills_from_job_names(df_jobs, top_n=20): # This function seems to be a precursor to the SKILL_REGEX_PATTERNS approach. Consider deprecating or aligning.
    if df_jobs.empty:
        return pd.DataFrame(columns=['skill', 'count'])

    # Ensure both columns exist, default to empty string if not
    job_names_series = df_jobs.get('job_name', pd.Series(dtype=str)).dropna().astype(str)
    major_required_series = df_jobs.get('major_required', pd.Series(dtype=str)).dropna().astype(str)

    if job_names_series.empty and major_required_series.empty:
        return pd.DataFrame(columns=['skill', 'count'])
            
    # More comprehensive list, consider moving to a config file or separate list
    keywords = [
        'Python', 'Java', 'Go', 'Golang', 'C++', 'C#', 'JavaScript', 'JS', 'TypeScript', 'TS', 
        'React', 'Vue', 'Angular', 'Node.js', 'Node', 'Spring', 'Django', 'Flask', 'FastAPI',
        'SQL', 'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQLServer', 'NoSQL',
        'AWS', 'Azure', 'GCP', 'é˜¿é‡Œäº‘', 'è…¾è®¯äº‘', 'åŽä¸ºäº‘', 'äº‘è®¡ç®—', 'äº‘åŽŸç”Ÿ', 'Cloud',
        'Docker', 'Kubernetes', 'K8S', 'CI/CD', 'DevOps', 'Jenkins', 'Git', 'Linux', 'Unix', 'Shell',
        'æ•°æ®åˆ†æž', 'æ•°æ®æŒ–æŽ˜', 'å¤§æ•°æ®', 'Spark', 'Hadoop', 'Flink', 'Kafka', 'Hive', 'HBase',
        'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'AI', 'äººå·¥æ™ºèƒ½', 'NLP', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'CV', 'è®¡ç®—æœºè§†è§‰', 'æŽ¨èç®—æ³•',
        'ç®—æ³•', 'æž¶æž„', 'æž¶æž„å¸ˆ', 'å‰ç«¯', 'åŽç«¯', 'å…¨æ ˆ', 'æµ‹è¯•', 'è½¯ä»¶æµ‹è¯•', 'è‡ªåŠ¨åŒ–æµ‹è¯•', 'æ€§èƒ½æµ‹è¯•', 'è¿ç»´', 'SRE',
        'äº§å“ç»ç†', 'é¡¹ç›®ç»ç†', 'PM', 'è¿è¥', 'å¸‚åœº', 'é”€å”®', 'BD', 'å®¢æœ', 'HR', 'äººäº‹', 'è¡Œæ”¿', 'è´¢åŠ¡', 'ä¼šè®¡', 'æ³•åŠ¡',
        'UI', 'UX', 'è®¾è®¡', 'å¹³é¢è®¾è®¡', 'äº¤äº’è®¾è®¡', 'æ¸¸æˆå¼€å‘', 'UE4', 'UE5', 'Unity', 'U3D',
        'ç§»åŠ¨å¼€å‘', 'iOS', 'Android', 'Flutter', 'React Native', 'å°ç¨‹åº', 'å†™ä½œ',
        'åµŒå…¥å¼', 'ç‰©è”ç½‘', 'IoT', 'èŠ¯ç‰‡', 'FPGA', 'é©±åŠ¨å¼€å‘', 'ç½‘ç»œå®‰å…¨', 'ä¿¡æ¯å®‰å…¨',
        # å¯ä»¥è€ƒè™‘åŠ å…¥ä¸€äº›ä¸Žä¸“ä¸šå¼ºç›¸å…³çš„é€šç”¨æŠ€æœ¯è¯ï¼Œä¾‹å¦‚ï¼š
        'è®¡ç®—æœº', 'è½¯ä»¶å·¥ç¨‹', 'é€šä¿¡å·¥ç¨‹', 'ç”µå­ä¿¡æ¯', 'è‡ªåŠ¨åŒ–', 'æœºæ¢°', 'æ•°å­¦', 'ç»Ÿè®¡' # è¿™äº›ä¹Ÿå¯èƒ½å‡ºçŽ°åœ¨ major_required
    ]
    
    escaped_keywords = []
    for kw in keywords:
        if kw.isalnum(): # For simple alphanumeric keywords
            # For Chinese keywords or mixed, \b might not work as expected with re.IGNORECASE
            # if re.search(r'[\u4e00-\u9fff]', kw): # If it contains Chinese
            #     escaped_keywords.append(re.escape(kw)) # Match exactly, no word boundaries
            # else:
            escaped_keywords.append(r'\b' + re.escape(kw) + r'\b')
        else: # For keywords with special characters like C++, C#, Node.js
            escaped_keywords.append(re.escape(kw))

    regex_pattern = r'(?:' + '|'.join(escaped_keywords) + r')'
    
    found_skills_raw = []
    
    # Process job_name
    for name_text in job_names_series:
        if pd.notna(name_text) and name_text.strip():
            found_skills_raw.extend(re.findall(regex_pattern, name_text, re.IGNORECASE))
    
    # Process major_required
    # Define terms in major_required that usually mean "no specific skill constraint" from this field
    generic_major_placeholders = {'ä¸é™', 'ä¸é™ä¸“ä¸š', 'ä¸“ä¸šä¸é™', 'æ— ä¸“ä¸šé™åˆ¶', 'ç›¸å…³ä¸“ä¸š', '', 'æ— '}
    for major_text in major_required_series:
        if pd.notna(major_text) and major_text.strip():
            # Avoid extracting from very generic major requirements
            cleaned_major_text = major_text.strip()
            if cleaned_major_text.lower() not in [p.lower() for p in generic_major_placeholders]:
                # Check if it doesn't solely consist of placeholders
                is_placeholder_only = True
                for placeholder in generic_major_placeholders:
                    if placeholder.lower() not in cleaned_major_text.lower():
                        is_placeholder_only = False
                        break
                if not is_placeholder_only or len(cleaned_major_text) > 5 : # Arbitrary length to allow short specific majors
                    found_skills_raw.extend(re.findall(regex_pattern, cleaned_major_text, re.IGNORECASE))
    
    if not found_skills_raw:
        return pd.DataFrame(columns=['skill', 'count'])

    normalized_skill_counts = Counter()
    keyword_lower_map = {kw.lower(): kw for kw in keywords} 

    for skill_match in found_skills_raw:
        # Normalize: try to match to original keyword casing, otherwise use the match itself
        # This handles cases where regex might pick up "python" but keyword is "Python"
        original_case_skill = keyword_lower_map.get(skill_match.lower(), skill_match)
        normalized_skill_counts[original_case_skill] += 1
            
    common_skills = normalized_skill_counts.most_common(top_n)
    return pd.DataFrame(common_skills, columns=['skill', 'count'])

# --- Plotting functions ---
def plot_bar_chart(df, x_col, y_col, title, x_label, y_label, color=None, orientation='v', text_auto=True):
    if df.empty or x_col not in df.columns or y_col not in df.columns:
        if streamlit_app_dir: st.write(f"å›¾è¡¨ '{title}' æ— æ•°æ®æ˜¾ç¤ºæˆ–ç¼ºå°‘å¿…è¦åˆ—ã€‚")
        return
    
    df_plot = df.copy()
    # Ensure data types are suitable for plotting
    if orientation == 'h':
        df_plot[x_col] = df_plot[x_col].astype(str) # Y-axis categories
        df_plot[y_col] = pd.to_numeric(df_plot[y_col], errors='coerce').fillna(0) # X-axis values
        fig = px.bar(df_plot, y=x_col, x=y_col, title=title, labels={x_col: x_label, y_col: y_label}, 
                     color=color, orientation='h', text_auto=text_auto if pd.api.types.is_numeric_dtype(df_plot[y_col]) else None)
        fig.update_yaxes(categoryorder='total ascending') # Sort bars by value
    else: # Vertical bar chart
        df_plot[x_col] = df_plot[x_col].astype(str) # X-axis categories
        df_plot[y_col] = pd.to_numeric(df_plot[y_col], errors='coerce').fillna(0) # Y-axis values
        fig = px.bar(df_plot, x=x_col, y=y_col, title=title, labels={x_col: x_label, y_col: y_label}, 
                     color=color, text_auto=text_auto if pd.api.types.is_numeric_dtype(df_plot[y_col]) else None)
        fig.update_layout(xaxis_tickangle=-45)
    if streamlit_app_dir: st.plotly_chart(fig, use_container_width=True)

def plot_pie_chart(df, names_col, values_col, title, hole=0.0): # Ensure hole is float
    if df.empty or names_col not in df.columns or values_col not in df.columns:
        if streamlit_app_dir: st.write(f"é¥¼å›¾ '{title}' æ— æ•°æ®æ˜¾ç¤ºæˆ–ç¼ºå°‘å¿…è¦åˆ—ã€‚")
        return
    df_plot = df.copy()
    df_plot[names_col] = df_plot[names_col].astype(str)
    df_plot[values_col] = pd.to_numeric(df_plot[values_col], errors='coerce').fillna(0)
    fig = px.pie(df_plot, names=names_col, values=values_col, title=title, hole=hole)
    if streamlit_app_dir: st.plotly_chart(fig, use_container_width=True)

def plot_line_chart(series, title, x_label="æ—¥æœŸ", y_label="æ•°é‡/å€¼", default_lookback_days=None):
    if not isinstance(series, pd.Series) or series.empty:
        if streamlit_app_dir: st.info(f"æŠ˜çº¿å›¾ '{title}' æ— æœ‰æ•ˆæ•°æ®å¯ä¾›å±•ç¤ºã€‚") 
        return

    plot_series = series.copy() 
    plot_series.index.name = x_label # Set index name for clearer hover label

    # Apply lookback if specified
    if default_lookback_days and not plot_series.empty and isinstance(plot_series.index, pd.DatetimeIndex):
        max_date_in_series = plot_series.index.max()
        if pd.notna(max_date_in_series): 
            cutoff_date = max_date_in_series - pd.Timedelta(days=default_lookback_days)
            min_date_in_series = plot_series.index.min()
            # Ensure cutoff is not before the actual start of data
            if pd.notna(min_date_in_series) and cutoff_date < min_date_in_series:
                cutoff_date = min_date_in_series 
            plot_series = plot_series[plot_series.index >= cutoff_date]
    
    if plot_series.empty: 
        if streamlit_app_dir: st.info(f"åœ¨é€‰å®šçš„å›žæº¯æœŸå†…ï¼Œå›¾è¡¨ '{title}' æ— æ•°æ®æ˜¾ç¤ºã€‚")
        return

    fig = px.line(plot_series, y=plot_series.name if plot_series.name else y_label, title=title, labels={'value': y_label}) # Use series name if available
    fig.update_xaxes(
        rangeslider_visible=True,
        rangeselector=dict(
            buttons=list([
                dict(count=1, label="è¿‘1æœˆ", step="month", stepmode="backward"),
                dict(count=3, label="è¿‘3æœˆ", step="month", stepmode="backward"),
                dict(count=6, label="è¿‘6æœˆ", step="month", stepmode="backward"),
                dict(count=1, label="ä»Šå¹´", step="year", stepmode="todate"),
                dict(count=1, label="è¿‘1å¹´", step="year", stepmode="backward"),
                dict(step="all", label="å…¨éƒ¨")
            ])
        )
    )
    if streamlit_app_dir: st.plotly_chart(fig, use_container_width=True)

def plot_scatter_mapbox(df, lat_col, lon_col, size_col=None, color_col=None, text_col=None, map_title="Job Distribution on Map"):
    if df.empty or lat_col not in df.columns or lon_col not in df.columns:
        if streamlit_app_dir: st.write("åœ°å›¾æ•°æ®ä¸å®Œæ•´æˆ–ç¼ºå°‘ç»çº¬åº¦åˆ—ã€‚")
        return
    
    df_map = df.copy()
    # Ensure lat/lon are numeric and drop NaNs
    df_map[lat_col] = pd.to_numeric(df_map[lat_col], errors='coerce')
    df_map[lon_col] = pd.to_numeric(df_map[lon_col], errors='coerce')
    df_map.dropna(subset=[lat_col, lon_col], inplace=True)

    if df_map.empty: 
        if streamlit_app_dir: st.write("æœªæ‰¾åˆ°æœ‰æ•ˆçš„åœ°ç†åæ ‡ç”¨äºŽåœ°å›¾ç»˜åˆ¶ã€‚"); return

    # Prepare size and color columns
    if size_col and size_col in df_map.columns: 
        df_map[size_col] = pd.to_numeric(df_map[size_col], errors='coerce').fillna(1)
    else: size_col = None # Ensure it's None if not valid

    if color_col and color_col in df_map.columns:
        df_map[color_col] = df_map[color_col].astype(str) # Treat color as categorical for map
        if df_map[color_col].nunique() > 20: # Limit distinct colors for clarity
             if streamlit_app_dir: st.info("é¢œè‰²ç¼–ç çš„ç±»åˆ«è¿‡å¤šï¼Œåœ°å›¾å°†ä¸ä½¿ç”¨é¢œè‰²åŒºåˆ†ã€‚")
             color_col = None 
    else: color_col = None


    hover_data_dict = {lat_col: False, lon_col: False} # Don't show lat/lon in hover by default
    custom_hover_cols = ['city_clean', 'province_clean', 'job_count', 'average_salary', 'median_salary'] 
    for h_col in custom_hover_cols:
        if h_col in df_map.columns:
            if pd.api.types.is_numeric_dtype(df_map[h_col]): # Check if column is numeric
                hover_data_dict[h_col] = ':.1f' if 'salary' in h_col else True # Format numerics
            else:
                hover_data_dict[h_col] = True # Show strings as is
    
    # Determine text to display on map markers
    effective_text_col = None
    if text_col and text_col in df_map.columns : effective_text_col = text_col
    elif 'city_clean' in df_map.columns : effective_text_col = 'city_clean'


    fig = px.scatter_mapbox(df_map, lat=lat_col, lon=lon_col,
                            size=size_col, color=color_col,
                            text=effective_text_col,
                            size_max=20 if size_col else 8, zoom=3, height=600, title=map_title,
                            hover_name=effective_text_col if effective_text_col else None, # Use determined text col for hover name
                            hover_data=hover_data_dict
                           )
    fig.update_layout(mapbox_style="carto-positron", margin={"r":0,"t":30,"l":0,"b":0})
    if streamlit_app_dir: st.plotly_chart(fig, use_container_width=True)


@st.cache_data
def calculate_time_deltas(df_jobs): # Already corrected in previous step
    if df_jobs.empty:
        return df_jobs.assign(time_delta_days=pd.NA, publish_to_update_hours=pd.NA, job_age_days=pd.NA)

    df = df_jobs.copy()
    date_cols_to_process = []

    for col_name in ['publish_date_dt', 'update_date_dt']:
        if col_name in df.columns:
            date_cols_to_process.append(col_name)
        else:
            df[col_name] = pd.NaT # Add as NaT if missing

    for col in date_cols_to_process:
        df[col] = pd.to_datetime(df[col], errors='coerce')
        if df[col].dt.tz is not None:
            df[col] = df[col].dt.tz_convert('UTC')
        else:
            df[col] = df[col].dt.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')

    df['time_delta_days'] = pd.NA
    df['publish_to_update_hours'] = pd.NA
    df['job_age_days'] = pd.NA

    if 'publish_date_dt' in df.columns and 'update_date_dt' in df.columns:
        valid_dates_mask = df['publish_date_dt'].notna() & df['update_date_dt'].notna()
        if valid_dates_mask.any():
            delta = (df.loc[valid_dates_mask, 'update_date_dt'] - df.loc[valid_dates_mask, 'publish_date_dt'])
            df.loc[valid_dates_mask, 'time_delta_days'] = delta.dt.total_seconds() / (24 * 60 * 60)
            df.loc[valid_dates_mask, 'publish_to_update_hours'] = delta.dt.total_seconds() / (60 * 60)

    now_utc = datetime.now(timezone.utc)
    if 'publish_date_dt' in df.columns:
        valid_publish_mask = df['publish_date_dt'].notna()
        if valid_publish_mask.any():
            df.loc[valid_publish_mask, 'job_age_days'] = (now_utc - df.loc[valid_publish_mask, 'publish_date_dt']).dt.total_seconds() / (24 * 60 * 60)
    
    if 'time_delta_days' in df.columns: df.loc[df['time_delta_days'] < 0, 'time_delta_days'] = pd.NA
    if 'publish_to_update_hours' in df.columns: df.loc[df['publish_to_update_hours'] < 0, 'publish_to_update_hours'] = pd.NA
    if 'job_age_days' in df.columns: df.loc[df['job_age_days'] < 0, 'job_age_days'] = pd.NA
    
    return df

@st.cache_data
def get_job_freshness_distribution(df_with_age, bins=None):
    if df_with_age.empty or 'job_age_days' not in df_with_age.columns:
        return pd.DataFrame(columns=['freshness_range', 'count'])
    
    # Define labels matching the number of bins - 1
    default_labels = ['æ— æ•ˆ/æœªæ¥', '1å¤©å†…', '1-3å¤©', '3-7å¤©', '1-2å‘¨', '2å‘¨-1ä¸ªæœˆ', '1-2ä¸ªæœˆ', '2-3ä¸ªæœˆ', '3-6ä¸ªæœˆ', 'åŠå¹´-1å¹´', '1å¹´ä»¥ä¸Š']
    if bins is None:
        # Bins: -inf (for <0), 0 (for exactly 0), 1 (for <1 day), 3, 7, 14, 30, 60, 90, 180, 365, +inf
        default_bins = [-float('inf'), 0, 1, 3, 7, 14, 30, 60, 90, 180, 365, float('inf')]
    else: # If custom bins are provided, ensure labels match
        default_labels = [f"Range {i+1}" for i in range(len(bins)-1)]


    job_age_days_series = pd.to_numeric(df_with_age['job_age_days'], errors='coerce')
    nan_count = job_age_days_series.isnull().sum()

    # Process only non-NaN values for pd.cut
    df_temp = pd.DataFrame({'job_age_days': job_age_days_series.dropna()})
    
    if df_temp.empty: # All were NaN or non-numeric
        if nan_count > 0: # If only NaNs, return a df with 'æœªçŸ¥' category
            return pd.DataFrame([{'freshness_range': 'æœªçŸ¥', 'count': nan_count}])
        return pd.DataFrame(columns=['freshness_range', 'count']) # No data at all

    # Apply pd.cut
    df_temp['freshness_range'] = pd.cut(
        df_temp['job_age_days'], 
        bins=default_bins, 
        labels=default_labels, 
        right=False,        # [lower, upper) interval
        include_lowest=True # Ensures the lowest value in bins is included
    )
    
    freshness_counts_df = df_temp['freshness_range'].value_counts().reset_index()
    freshness_counts_df.columns = ['freshness_range', 'count']
    
    if nan_count > 0: # Add NaN count as 'æœªçŸ¥' if there were any
        unknown_df = pd.DataFrame([{'freshness_range': 'æœªçŸ¥', 'count': nan_count}])
        freshness_counts_df = pd.concat([freshness_counts_df, unknown_df], ignore_index=True)

    # Set categorical order for proper sorting in charts
    final_labels_for_ordering = default_labels[:]
    if nan_count > 0 and 'æœªçŸ¥' not in final_labels_for_ordering : 
        final_labels_for_ordering.append('æœªçŸ¥')
    
    # Filter categories to only those present in the data to avoid issues with pd.Categorical
    present_categories = [l for l in final_labels_for_ordering if l in freshness_counts_df['freshness_range'].unique()]
    
    freshness_counts_df['freshness_range'] = pd.Categorical(
        freshness_counts_df['freshness_range'], 
        categories=present_categories, 
        ordered=True
    )
    return freshness_counts_df.sort_values('freshness_range')