import sys
import os
import threading # ç”¨äºå¼‚æ­¥çˆ¬å–
import subprocess
import json
import time
import pandas as pd
from datetime import datetime

# --- sys.path å’Œè·¯å¾„å®šä¹‰ (ä¿æŒä¸å˜) ---
# streamlit_app_dir: E:\MyProjects\pythonfinishwork\streamlit_app
streamlit_app_dir = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(streamlit_app_dir)
SCRAPY_PROJECT_ROOT = os.path.join(PROJECT_ROOT, 'crawler')
SCRAPY_MODULE_DIR = os.path.join(SCRAPY_PROJECT_ROOT, 'crawler')
DATA_DIR = os.path.join(SCRAPY_MODULE_DIR, 'data') # è¿™ä¸ªDATA_DIRæ˜¯Scrapyé¡¹ç›®å†…éƒ¨çš„
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

import streamlit as st
from streamlit_app.utils import (
    JOBS_FILE, DATA_DIR_INSIDE_CRAWLER, # DATA_DIR_INSIDE_CRAWLER åº”è¯¥æ˜¯ utils.py ä¸­å®šä¹‰çš„æŒ‡å‘ Scrapy é¡¹ç›®æ•°æ®ç›®å½•çš„è·¯å¾„
    load_json_data, preprocess_jobs_data,
    get_top_n_counts, get_average_salary,
    get_time_series_data, plot_line_chart,
    plot_bar_chart, plot_pie_chart,
    load_scrapy_default_targets,
    extract_skills_from_job_names # ç¡®è®¤è¿™ä¸ªå‡½æ•°æ˜¯å¦ä»ç„¶éœ€è¦ï¼Œæˆ–è€…ä½¿ç”¨åœ¨preprocess_jobs_dataä¸­ç”Ÿæˆçš„extracted_skills_list
)
import plotly.express as px

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="æ‹›è˜å¸‚åœºæ´å¯Ÿä¸å®æ—¶çˆ¬å–",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={'About': "# æ‹›è˜å¸‚åœºåˆ†æä¸å®æ—¶çˆ¬å–ä»ªè¡¨ç›˜"}
)

# --- åŠ è½½ä¸»æ•°æ®å’Œé»˜è®¤çˆ¬å–é€‰é¡¹ (ç¼“å­˜) ---
@st.cache_data(ttl=3600)
def load_main_app_data():
    df_jobs_raw = load_json_data(JOBS_FILE)
    df_jobs = preprocess_jobs_data(df_jobs_raw) if not df_jobs_raw.empty else pd.DataFrame()
    
    try:
        default_targets = load_scrapy_default_targets()
    except Exception as e:
        print(f"CRITICAL WARNING in app.py: Failed to load Scrapy default targets from utils: {e}")
        # æä¾›ä¸€ä¸ªç»å¯¹æœ€å°çš„ fallbackï¼Œç¡®ä¿å­—å…¸ç»“æ„å­˜åœ¨
        default_targets = {
            "cities": [{"name": "ğŸŒ ä¸é™çœä»½/åœ°åŒº", "code": ""}],
            "categories": [{"name": "ğŸ“š ä¸é™ç±»åˆ«", "code": ""}],
            "industries": [{"name": "ğŸ­ ä¸é™è¡Œä¸š", "code": ""}],
            "workExperiences": [{"name": "â³ ä¸é™å·¥ä½œç»éªŒ", "code": ""}],
            "degrees": [{"name": "ğŸ“ ä¸é™å­¦å†", "code": ""}],
            "scales": [{"name": "âš–ï¸ ä¸é™å…¬å¸è§„æ¨¡", "code": ""}],
            "corpProps": [{"name": "ğŸ›ï¸ ä¸é™å…¬å¸æ€§è´¨", "code": ""}]
        }
        # å¯ä»¥è€ƒè™‘åœ¨è¿™é‡Œæ·»åŠ ä¸€äº›å…·ä½“çš„é€‰é¡¹ä½œä¸ºåå¤‡ï¼Œå¦‚æœä¸Šé¢çš„ utils åŠ è½½å®Œå…¨å¤±è´¥
        default_targets['cities'].extend([{"name": "åŒ—äº¬å¸‚", "code": "11"}, {"name": "ä¸Šæµ·å¸‚", "code": "31"}])
        default_targets['categories'].extend([{"name": "è®¡ç®—æœºè½¯ä»¶", "code": "010000"}, {"name": "äº’è”ç½‘/ç”µå­å•†åŠ¡", "code": "000104"}])

    return df_jobs, default_targets

df_jobs_main, default_crawl_targets = load_main_app_data()

# --- Session State åˆå§‹åŒ– (ä¿æŒä¸å˜) ---
if 'realtime_crawl_results_df' not in st.session_state:
    st.session_state.realtime_crawl_results_df = pd.DataFrame()
if 'realtime_crawl_message' not in st.session_state:
    st.session_state.realtime_crawl_message = ""
if 'is_crawling' not in st.session_state:
    st.session_state.is_crawling = False
if 'crawl_process_info' not in st.session_state:
    st.session_state.crawl_process_info = None
if 'crawl_output_file_path_session' not in st.session_state:
    st.session_state.crawl_output_file_path_session = None

# --- ä¾§è¾¹æ  (ä¿æŒä¸å˜) ---
st.sidebar.title("ğŸ§­ å¯¼èˆªä¸çŠ¶æ€") # Added Emoji
st.sidebar.info("åœ¨æ­¤é¡µé¢è¿›è¡Œå®æ—¶å²—ä½çˆ¬å–ï¼Œæˆ–é€šè¿‡å…¶ä»–é¡µé¢æ¢ç´¢å†å²æ•°æ®æ´å¯Ÿã€‚") # Adjusted text
st.sidebar.markdown("---")
if os.path.exists(JOBS_FILE):
    try:
        last_updated_time = datetime.fromtimestamp(os.path.getmtime(JOBS_FILE)).strftime('%Y-%m-%d %H:%M:%S')
        st.sidebar.caption(f"ä¸»è¦æ•°æ®æ–‡ä»¶æœ€åæ›´æ–°äº:\nğŸ—“ï¸ {last_updated_time}") # Added Emoji
    except Exception: pass
else:
    st.sidebar.warning(f"âš ï¸ æ ¸å¿ƒæ•°æ®æ–‡ä»¶ ({os.path.basename(JOBS_FILE)}) æœªæ‰¾åˆ°ã€‚") # Added Emoji

# --- ä¸»é¡µé¢å†…å®¹ ---
st.title("ğŸ“Š æ‹›è˜å¸‚åœºæ´å¯Ÿä»ªè¡¨ç›˜") # Simplified title
st.markdown("""
æ¬¢è¿ä½¿ç”¨æœ¬ä»ªè¡¨ç›˜ï¼æ‚¨å¯ä»¥åœ¨æ­¤ï¼š
- **ğŸš€ å®æ—¶çˆ¬å–**æœ€æ–°çš„å²—ä½æ•°æ®å¹¶è¿›è¡Œå³æ—¶åˆ†æã€‚
- **ğŸ“ˆ æ¢ç´¢å†å²æ•°æ®**ï¼Œé€šè¿‡å¤šä¸ªç»´åº¦æ´å¯Ÿæ‹›è˜å¸‚åœºè¶‹åŠ¿ã€‚
""")
st.divider()

# --- å¼‚æ­¥çˆ¬å–å‡½æ•° (ä¿æŒä¸å˜) ---
def run_scrapy_in_thread(scrapy_cmd_args_list, crawler_root_path, output_file_abs_path_for_thread):
    st.session_state.is_crawling = True
    st.session_state.crawl_process_info = None
    st.session_state.crawl_output_file_path_session = output_file_abs_path_for_thread
    print(f"\n[{datetime.now()}] THREAD: Launching Scrapy command:")
    print(f"Executing: {' '.join(scrapy_cmd_args_list)}")
    print(f"In CWD: {crawler_root_path}")
    print(f"Outputting Scrapy items to: {output_file_abs_path_for_thread}\n")
    process = None
    try:
        process = subprocess.Popen(
            scrapy_cmd_args_list, cwd=crawler_root_path,
            stdout=None, stderr=subprocess.PIPE, text=True, encoding='utf-8', creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0 # Hides console window on Windows
        )
        try:
            _, stderr_output = process.communicate(timeout=180) # 3 minutes timeout
            returncode = process.returncode
        except subprocess.TimeoutExpired:
            print(f"[{datetime.now()}] THREAD: Scrapy process timed out. Terminating...")
            process.terminate()
            try: process.wait(timeout=10)
            except subprocess.TimeoutExpired: process.kill()
            returncode = -99 # Custom code for timeout
            stderr_output = "çˆ¬å–æ“ä½œè¶…æ—¶ (è¶…è¿‡3åˆ†é’Ÿ)ã€‚"
        st.session_state.crawl_process_info = {'stderr': stderr_output, 'returncode': returncode}
    except FileNotFoundError:
        print(f"[{datetime.now()}] THREAD: Scrapy command not found.")
        st.session_state.crawl_process_info = {'error_type': 'filenotfound', 'stderr': 'Scrapy å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿ Scrapy å·²å®‰è£…å¹¶é…ç½®åœ¨ç³»ç»Ÿè·¯å¾„ä¸­ã€‚'}
    except Exception as e:
        print(f"[{datetime.now()}] THREAD: Exception during Scrapy Popen/communicate: {e}")
        st.session_state.crawl_process_info = {'error_type': 'exception', 'stderr': f'æ‰§è¡Œå®æ—¶çˆ¬å–æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}'}
    finally:
        st.session_state.is_crawling = False
        st.rerun() # Trigger a rerun to update the UI


# --- å®æ—¶çˆ¬å–æ¨¡å— ---
st.header("ğŸš€ å®æ—¶å²—ä½æ•°æ®çˆ¬å–ä¸åˆ†æ")
with st.expander("å±•å¼€è¿›è¡Œå®æ—¶çˆ¬å–", expanded=True):
    with st.form(key="realtime_crawl_form_main_v6"): # Incremented key
        st.markdown("**ğŸ¯ æ ¹æ®æ‚¨çš„éœ€æ±‚å®šåˆ¶çˆ¬å–ç›®æ ‡:**")
        
        # ç¬¬ä¸€è¡Œç­›é€‰ï¼šå…³é”®è¯ï¼Œçœä»½ï¼Œç±»åˆ« (ä¿æŒä¸å˜æˆ–å¾®è°ƒ)
        form_row1_col1, form_row1_col2, form_row1_col3 = st.columns([2,2,2])
        with form_row1_col1:
            rt_keyword_input = st.text_input("ğŸ“ è¾“å…¥èŒä½å…³é”®è¯ (å¯é€‰)", placeholder="ä¾‹å¦‚: python, æ•°æ®åˆ†æ", key="rt_keyword_input_v6")
        with form_row1_col2:
            city_options_list_rt = default_crawl_targets.get('cities', [])
            city_display_names_rt = ["ğŸŒ ä¸é™çœä»½/åœ°åŒº"] + sorted(list(set(c['name'] for c in city_options_list_rt if c.get('name'))))
            default_city_index_rt = 0
            rt_selected_city_name_form = st.selectbox("ğŸ“ é€‰æ‹©ç›®æ ‡çœä»½/åœ°åŒº:", city_display_names_rt, index=default_city_index_rt, key="rt_city_select_v6")
        with form_row1_col3:
            category_options_list_rt = default_crawl_targets.get('categories', [])
            category_display_names_rt = ["ğŸ“š ä¸é™ç±»åˆ«"] + sorted(list(set(c['name'] for c in category_options_list_rt if c.get('name'))))
            default_cat_index_rt = 0
            rt_selected_category_name_form = st.selectbox("ğŸ·ï¸ é€‰æ‹©ç›®æ ‡èŒä½ç±»åˆ«:", category_display_names_rt, index=default_cat_index_rt, key="rt_category_select_v6")

        st.markdown("---") # åˆ†éš”ç¬¦ï¼Œæ›´æ¸…æ™°
        st.markdown("**æ›´å¤šç­›é€‰æ¡ä»¶ (å¯é€‰):**")
        
        # ç¬¬äºŒè¡Œç­›é€‰ï¼šè¡Œä¸šï¼Œå·¥ä½œç»éªŒ
        form_row2_col1, form_row2_col2 = st.columns(2)
        with form_row2_col1:
            industry_options_list_rt = default_crawl_targets.get('industries', []) # å‡è®¾ utils å·²æ›´æ–°
            industry_display_names_rt = ["ğŸ­ ä¸é™è¡Œä¸š"] + sorted(list(set(i['name'] for i in industry_options_list_rt if i.get('name'))))
            default_industry_index_rt = 0
            rt_selected_industry_name_form = st.selectbox(
                "ğŸ¢ é€‰æ‹©ç›®æ ‡è¡Œä¸š:", 
                industry_display_names_rt, 
                index=default_industry_index_rt, 
                key="rt_industry_select_v6"
            )
        with form_row2_col2:
            work_exp_options_list_rt = default_crawl_targets.get('workExperiences', [])
            work_exp_display_names_rt = [item['name'] for item in work_exp_options_list_rt if item.get('name')]
            rt_selected_work_exp_name_form = st.selectbox(
                "ğŸ› ï¸ é€‰æ‹©å·¥ä½œç»éªŒè¦æ±‚:", 
                options=work_exp_display_names_rt,
                index=0,
                key="rt_work_exp_select_v6_new"
            )
            
        # ç¬¬ä¸‰è¡Œç­›é€‰ï¼šå­¦å†ï¼Œå…¬å¸è§„æ¨¡ï¼Œå…¬å¸æ€§è´¨
        form_row3_col1, form_row3_col2, form_row3_col3 = st.columns(3)
        with form_row3_col1:
            # å­¦å†
            degree_options_list_rt = default_crawl_targets.get('degrees', []) 
            degree_display_names_rt = [item['name'] for item in degree_options_list_rt if item.get('name')]
            # default_degree_index_rt åº”è¯¥æ€»æ˜¯ 0ï¼Œå› ä¸º "ä¸é™å­¦å†" ä¿è¯æ˜¯ç¬¬ä¸€ä¸ª
            rt_selected_degree_name_form = st.selectbox(
                "ğŸ“œ é€‰æ‹©å­¦å†è¦æ±‚:", 
                options=degree_display_names_rt, 
                index=0, 
                key="rt_degree_select_v6_new" # å»ºè®®æ›´æ–°keyä»¥ä¾¿åˆ·æ–°
            )
        with form_row3_col2:
            # å…¬å¸è§„æ¨¡
            scale_options_list_rt = default_crawl_targets.get('scales', [])
            scale_display_names_rt = [item['name'] for item in scale_options_list_rt if item.get('name')]
            rt_selected_scale_name_form = st.selectbox(
                "ğŸ“ˆ é€‰æ‹©å…¬å¸è§„æ¨¡:", 
                options=scale_display_names_rt,
                index=0,
                key="rt_scale_select_v6_new"
            )
        with form_row3_col3:
            property_options_list_rt = default_crawl_targets.get('corpProps', []) # å‡è®¾ utils å·²æ›´æ–° (corpProps æ˜¯å¸¸ç”¨é”®å)
            property_display_names_rt = ["ğŸ›ï¸ ä¸é™å…¬å¸æ€§è´¨"] + sorted(list(set(p['name'] for p in property_options_list_rt if p.get('name'))))
            default_property_index_rt = 0
            rt_selected_property_name_form = st.selectbox(
                "âš–ï¸ é€‰æ‹©å…¬å¸æ€§è´¨:", # Emoji may need adjustment
                property_display_names_rt, 
                index=default_property_index_rt, 
                key="rt_property_select_v6"
            )

        rt_submit_button = st.form_submit_button(label="âš¡ å¼€å§‹å®æ—¶çˆ¬å–åˆ†æ", type="primary", use_container_width=True, disabled=st.session_state.is_crawling)

    if rt_submit_button:
        if not st.session_state.is_crawling:
            st.session_state.is_crawling = True
            st.session_state.realtime_crawl_message = "â³ æ­£åœ¨å¯åŠ¨å®æ—¶çˆ¬å–ï¼Œè¯·ç¨å€™... (è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹ç»ˆç«¯)"
            st.session_state.realtime_crawl_results_df = pd.DataFrame()
            st.session_state.crawl_process_info = None

            # --- æ„é€  Scrapy å‘½ä»¤å‚æ•° ---
            scrapy_cmd_args = ['scrapy', 'crawl', 'jobs', '-a', f'run_type=realtime_app_home_v5'] # æ›´æ–° run_type ç‰ˆæœ¬

            # å…³é”®è¯
            if rt_keyword_input.strip(): 
                scrapy_cmd_args.extend(['-a', f'target_keywords_str={rt_keyword_input.strip()}'])

            # çœä»½/åœ°åŒº (target_cities_json)
            target_cities_param_list = []
            if rt_selected_city_name_form != "ğŸŒ ä¸é™çœä»½/åœ°åŒº":
                city_code = next((c['code'] for c in default_crawl_targets.get('cities', []) if c['name'] == rt_selected_city_name_form), None)
                if city_code is not None: 
                    target_cities_param_list = [{"code": city_code, "name": rt_selected_city_name_form}]
                    scrapy_cmd_args.extend(['-a', f'target_cities_json={json.dumps(target_cities_param_list)}'])
            
            # èŒä½ç±»åˆ« (target_categories_json)
            target_categories_param_list = []
            if rt_selected_category_name_form != "ğŸ“š ä¸é™ç±»åˆ«":
                cat_code = next((c['code'] for c in default_crawl_targets.get('categories', []) if c['name'] == rt_selected_category_name_form), None)
                if cat_code is not None: 
                    target_categories_param_list = [{"code": cat_code, "name": rt_selected_category_name_form}]
                    scrapy_cmd_args.extend(['-a', f'target_categories_json={json.dumps(target_categories_param_list)}'])

            # è¡Œä¸š (target_industries_json)
            if rt_selected_industry_name_form != "ğŸ­ ä¸é™è¡Œä¸š":
                industry_code = next((i['code'] for i in default_crawl_targets.get('industries', []) if i['name'] == rt_selected_industry_name_form), None)
                if industry_code is not None:
                    # å‡è®¾çˆ¬è™«æœŸæœ›çš„å‚æ•°åæ˜¯ target_industries_json æˆ–ç±»ä¼¼
                    scrapy_cmd_args.extend(['-a', f'target_industries_json={json.dumps([{"code": industry_code, "name": rt_selected_industry_name_form}])}'])
            
            # å·¥ä½œç»éªŒ (target_workexp_json or target_workexp_code)
            if rt_selected_work_exp_name_form != "â³ ä¸é™å·¥ä½œç»éªŒ":
                work_exp_code = next((w['code'] for w in default_crawl_targets.get('workExperiences', []) if w['name'] == rt_selected_work_exp_name_form), None)
                if work_exp_code is not None:
                    # å‡è®¾çˆ¬è™«æœŸæœ›çš„å‚æ•°åæ˜¯ target_workexp_code æˆ– target_workexp_json
                    scrapy_cmd_args.extend(['-a', f'target_workexp_code={work_exp_code}']) 
                    # æˆ–è€… scrapy_cmd_args.extend(['-a', f'target_workexp_json={json.dumps([{"code": work_exp_code, "name": rt_selected_work_exp_name_form}])}'])

            # å­¦å† (target_degree_json or target_degree_code)
            if rt_selected_degree_name_form != "ğŸ“ ä¸é™å­¦å†":
                degree_code = next((d['code'] for d in default_crawl_targets.get('degrees', []) if d['name'] == rt_selected_degree_name_form), None)
                if degree_code is not None:
                    scrapy_cmd_args.extend(['-a', f'target_degree_code={degree_code}'])

            # å…¬å¸è§„æ¨¡ (target_scale_json or target_scale_code)
            if rt_selected_scale_name_form != "âš–ï¸ ä¸é™å…¬å¸è§„æ¨¡":
                scale_code = next((s['code'] for s in default_crawl_targets.get('scales', []) if s['name'] == rt_selected_scale_name_form), None)
                if scale_code is not None:
                    scrapy_cmd_args.extend(['-a', f'target_scale_code={scale_code}'])

            # å…¬å¸æ€§è´¨ (target_property_json or target_property_code)
            if rt_selected_property_name_form != "ğŸ›ï¸ ä¸é™å…¬å¸æ€§è´¨":
                prop_code = next((p['code'] for p in default_crawl_targets.get('corpProps', []) if p['name'] == rt_selected_property_name_form), None)
                if prop_code is not None:
                    scrapy_cmd_args.extend(['-a', f'target_property_code={prop_code}'])
            
            # --- è¾“å‡ºæ–‡ä»¶å’Œæ‰§è¡Œçº¿ç¨‹ (ä¸ä¹‹å‰ç±»ä¼¼) ---
            timestamp_str = str(int(time.time()))
            REALTIME_OUTPUT_FILENAME = f"jobs_realtime_{timestamp_str}.jsonl"
            REALTIME_OUTPUT_FILE_ABS_PATH = os.path.join(DATA_DIR_INSIDE_CRAWLER, REALTIME_OUTPUT_FILENAME)
            
            relative_output_path_for_scrapy_o = os.path.join('crawler', 'data', REALTIME_OUTPUT_FILENAME)
            scrapy_cmd_args.extend(['-o', relative_output_path_for_scrapy_o, '-L', 'INFO'])
            os.makedirs(os.path.dirname(REALTIME_OUTPUT_FILE_ABS_PATH), exist_ok=True)

            CRAWLER_PROJECT_ROOT_APP = SCRAPY_PROJECT_ROOT

            thread = threading.Thread(target=run_scrapy_in_thread, args=(scrapy_cmd_args, CRAWLER_PROJECT_ROOT_APP, REALTIME_OUTPUT_FILE_ABS_PATH))
            thread.start()
            st.rerun()

# --- å¤„ç†å’Œæ˜¾ç¤ºå®æ—¶çˆ¬å–çº¿ç¨‹çš„ç»“æœ ---
if st.session_state.is_crawling:
    st.info("âš™ï¸ **å®æ—¶çˆ¬å–æ­£åœ¨è¿›è¡Œä¸­...** è¯·æŸ¥çœ‹è¿è¡Œ Streamlit çš„ç»ˆç«¯è·å–è¯¦ç»†çš„ Scrapy æ—¥å¿—ã€‚å®Œæˆåæ­¤åŒºåŸŸä¼šè‡ªåŠ¨æ›´æ–°ã€‚")
elif st.session_state.get('crawl_process_info') is not None:
    process_info = st.session_state.crawl_process_info
    output_file = st.session_state.get('crawl_output_file_path_session')
    
    # ç»Ÿä¸€å¤„ç†æ¶ˆæ¯
    current_message = ""
    success_flag = False

    if process_info.get('error_type') == 'timeout': current_message = "âŒ å®æ—¶çˆ¬å–è¶…æ—¶ (è¶…è¿‡3åˆ†é’Ÿ)ã€‚"
    elif process_info.get('error_type') == 'filenotfound': current_message = "âŒ Scrapy å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®ã€‚"
    elif process_info.get('error_type'): current_message = f"âŒ æ‰§è¡Œçˆ¬å–æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {process_info.get('error_type')}"
    elif process_info.get('returncode') == 0:
        if output_file and os.path.exists(output_file):
            df_rt_raw = load_json_data(output_file) # load_json_data åº”è¯¥èƒ½å¤„ç† .jsonl
            if not df_rt_raw.empty:
                st.session_state.realtime_crawl_results_df = preprocess_jobs_data(df_rt_raw)
                current_message = f"âœ… çˆ¬å–å®Œæˆï¼æ‰¾åˆ° {len(st.session_state.realtime_crawl_results_df)} æ¡å²—ä½ã€‚"
                success_flag = True
            else:
                current_message = f"â„¹ï¸ çˆ¬å–æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªèƒ½ä»ä¸´æ—¶è¾“å‡ºæ–‡ä»¶ '{os.path.basename(output_file)}' è§£æåˆ°æœ‰æ•ˆæ•°æ®ã€‚"
            try: 
                if os.path.exists(output_file): os.remove(output_file) # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            except Exception as e_rm: print(f"Error removing temp file {output_file}: {e_rm}")
        else:
            current_message = f"âš ï¸ çˆ¬å–æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªæ‰¾åˆ°é¢„æœŸçš„è¾“å‡ºæ–‡ä»¶ '{os.path.basename(output_file if output_file else 'N/A')}'ã€‚"
    else:
        current_message = f"âŒ çˆ¬å–å¤±è´¥ (é”™è¯¯ä»£ç : {process_info.get('returncode')})ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹ä¸‹æ–¹é”™è¯¯è¾“å‡ºã€‚"
    
    st.session_state.realtime_crawl_message = current_message

    if process_info.get('stderr') and not success_flag : # åªåœ¨æœ‰é”™è¯¯ä¸”çˆ¬å–ä¸å®Œå…¨æˆåŠŸæ—¶é»˜è®¤å±•å¼€
        with st.expander("æŸ¥çœ‹çˆ¬è™«é”™è¯¯è¾“å‡º (stderr)", expanded=True): # Expanded if error
            st.text_area("Scrapy æ ‡å‡†é”™è¯¯:", value=process_info['stderr'], height=150, key="stderr_display_v3")
            
    st.session_state.crawl_process_info = None # æ¸…ç†ï¼Œé¿å…é‡å¤å¤„ç†
    st.session_state.crawl_output_file_path_session = None
    # ä¸éœ€è¦ st.rerun() åœ¨è¿™é‡Œï¼Œå› ä¸º run_scrapy_in_thread çš„ finally ä¸­å·²ç»æœ‰äº†

# --- æ˜¾ç¤ºå®æ—¶çˆ¬å–çš„æ¶ˆæ¯å’Œç»“æœå›¾è¡¨ ---
if st.session_state.realtime_crawl_message and not st.session_state.is_crawling:
    st.subheader("ğŸ“¡ å®æ—¶çˆ¬å–çŠ¶æ€ä¸ç»“æœ")
    if "âœ…" in st.session_state.realtime_crawl_message : st.success(st.session_state.realtime_crawl_message)
    elif "âŒ" in st.session_state.realtime_crawl_message : st.error(st.session_state.realtime_crawl_message)
    else: st.info(st.session_state.realtime_crawl_message)

    if not st.session_state.realtime_crawl_results_df.empty:
        df_rt_display = st.session_state.realtime_crawl_results_df
        rt_metric_col1, rt_metric_col2 = st.columns(2)
        with rt_metric_col1:
            st.metric("ğŸ“ˆ æ‰¾åˆ°å²—ä½æ•°", f"{len(df_rt_display):,}")
        with rt_metric_col2:
            rt_avg_salary_df = df_rt_display[df_rt_display['avg_month_pay'] > 0]
            rt_avg_sal = rt_avg_salary_df['avg_month_pay'].mean() if not rt_avg_salary_df.empty else 0
            st.metric("ğŸ’° å¹³å‡æœˆè–ª (K)", f"{rt_avg_sal:,.1f}" if rt_avg_sal > 0 else "N/A")

        rt_chart_col1, rt_chart_col2 = st.columns(2)
        with rt_chart_col1:
            st.markdown("###### ğŸ“ å­¦å†è¦æ±‚åˆ†å¸ƒ")
            rt_degrees_df = get_top_n_counts(df_rt_display, 'degree_name_cat', 5) # ä½¿ç”¨ degree_name_cat
            if not rt_degrees_df.empty:
                plot_pie_chart(rt_degrees_df, 'degree_name_cat', 'count', "å®æ—¶ç»“æœ-å­¦å†è¦æ±‚", hole=0.4) # Increased hole
            else: st.caption("æ— å­¦å†æ•°æ®ã€‚")
        with rt_chart_col2:
            st.markdown("###### ğŸ·ï¸ ä¸»è¦èŒä½ç±»åˆ«åˆ†å¸ƒ")
            top_cats_rt = get_top_n_counts(df_rt_display, 'job_catory', 5)
            if not top_cats_rt.empty:
                plot_pie_chart(top_cats_rt, 'job_catory', 'count', "å®æ—¶ç»“æœ-èŒä½ç±»åˆ«", hole=0.4)
            else: st.caption("æ— èŒä½ç±»åˆ«æ•°æ®ã€‚")
        
        # æŠ€èƒ½æå–ï¼Œéœ€è¦ç¡®è®¤ df_rt_display ä¸­æ˜¯å¦æœ‰ 'extracted_skills_list'
        if 'extracted_skills_list' in df_rt_display.columns:
            from streamlit_app.utils import get_skill_frequency # ç¡®ä¿å¯¼å…¥
            st.markdown("###### ğŸ› ï¸ çƒ­é—¨æŠ€èƒ½ (Top 5)")
            # å‡è®¾ get_skill_frequency å¯ä»¥å¤„ç† 'extracted_skills_list' åˆ—
            rt_skills_df = get_skill_frequency(df_rt_display, 'extracted_skills_list', top_n=5)
            if not rt_skills_df.empty:
                plot_bar_chart(rt_skills_df, 'skill', 'count', "å®æ—¶ç»“æœ-ä¸»è¦æŠ€èƒ½", "æŠ€èƒ½", "é¢‘æ¬¡", orientation='h')
            else: st.caption("æ— æŠ€èƒ½æ•°æ®æˆ–æœªèƒ½æå–ã€‚")
        else: # Fallback or if 'extract_skills_from_job_names' is preferred for this quick view
            st.markdown("###### ğŸ› ï¸ çƒ­é—¨æŠ€èƒ½ (Top 5 - åŸºäºèŒä½åç§°)")
            rt_skills_df_legacy = extract_skills_from_job_names(df_rt_display, top_n=5) # ç¡®è®¤æ­¤å‡½æ•°æ˜¯å¦ä»ç„¶é€‚ç”¨
            if not rt_skills_df_legacy.empty:
                 plot_bar_chart(rt_skills_df_legacy, 'skill' if 'skill' in rt_skills_df_legacy.columns else 'term', 'count', "å®æ—¶ç»“æœ-ä¸»è¦æŠ€èƒ½", "æŠ€èƒ½", "é¢‘æ¬¡", orientation='h')
            else: st.caption("æ— æŠ€èƒ½æ•°æ®ã€‚")


        if not rt_avg_salary_df.empty:
            st.subheader("å®æ—¶ç»“æœ - è–ªèµ„åˆ†å¸ƒç›´æ–¹å›¾")
            # å¢åŠ è–ªèµ„åˆ†ç®±ä»¥è·å¾—æ›´ç»†è‡´çš„è§†å›¾
            salary_bins_rt = [0, 5, 10, 15, 20, 25, 30, 40, 50, 200] # Max 200k for display
            salary_labels_rt = [f"{salary_bins_rt[i]}-{salary_bins_rt[i+1]}K" for i in range(len(salary_bins_rt)-1)]
            
            # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬è¿›è¡Œåˆ†ç®±ï¼Œé¿å…ä¿®æ”¹åŸå§‹ session_state DataFrame
            df_rt_display_for_hist = rt_avg_salary_df.copy()
            df_rt_display_for_hist['salary_group_rt'] = pd.cut(df_rt_display_for_hist['avg_month_pay'], bins=salary_bins_rt, labels=salary_labels_rt, right=False)
            
            # ç»Ÿè®¡æ¯ä¸ªè–ªèµ„ç»„çš„æ•°é‡
            salary_group_counts_rt = df_rt_display_for_hist['salary_group_rt'].value_counts().reset_index()
            salary_group_counts_rt.columns = ['salary_group_rt', 'count']
            # ç¡®ä¿è–ªèµ„ç»„æ˜¯Categoricalå¹¶æŒ‰å®šä¹‰çš„é¡ºåºæ’åº
            salary_group_counts_rt['salary_group_rt'] = pd.Categorical(salary_group_counts_rt['salary_group_rt'], categories=salary_labels_rt, ordered=True)
            salary_group_counts_rt.sort_values('salary_group_rt', inplace=True)


            fig_hist_rt = px.bar(salary_group_counts_rt, x="salary_group_rt", y="count", 
                                 title="å®æ—¶ç»“æœ - å¹³å‡æœˆè–ªåˆ†å¸ƒ (K/æœˆ)", 
                                 labels={'salary_group_rt': 'å¹³å‡æœˆè–ªèŒƒå›´ (K)', 'count': 'å²—ä½æ•°é‡'},
                                 text_auto=True)
            fig_hist_rt.update_layout(bargap=0.2)
            st.plotly_chart(fig_hist_rt, use_container_width=True)

        with st.expander("ğŸ“‹ æŸ¥çœ‹å®æ—¶çˆ¬å–æ•°æ®æ ·æœ¬ (æœ€å¤š100æ¡)", expanded=False):
            display_cols_rt = ['job_name', 'company_name', 'province_clean', 'city_clean', 'avg_month_pay', 'degree_name_cat', 'work_year_cat']
            # ç¡®ä¿è¿™äº›åˆ—éƒ½å­˜åœ¨äº df_rt_display
            valid_display_cols_rt = [col for col in display_cols_rt if col in df_rt_display.columns]
            st.dataframe(df_rt_display[valid_display_cols_rt].head(100), use_container_width=True) # Added use_container_width
    st.divider()


# --- ä¸»æ•°æ®æ¦‚è§ˆéƒ¨åˆ† ---
st.header("ğŸ“œ ä¸»æ•°æ®åº“å†å²æ•°æ®æ¦‚è§ˆ") # Added Emoji
if not df_jobs_main.empty:
    st.markdown("#### å…³é”®æŒ‡æ ‡")
    main_col1_metrics, main_col2_metrics, main_col3_metrics, main_col4_metrics = st.columns(4)
    main_col1_metrics.metric("ğŸ•’ å†å²æ€»å²—ä½æ•°", f"{len(df_jobs_main):,}")
    # ä½¿ç”¨ province_clean ç»Ÿè®¡çœä»½æ•°
    main_col2_metrics.metric("ğŸ—ºï¸ æ¶‰åŠçœä»½æ•°", df_jobs_main['province_clean'].nunique())
    main_col3_metrics.metric("ğŸ·ï¸ èŒä½ç±»åˆ«æ•°", df_jobs_main['job_catory'].nunique())
    
    avg_salary_overall_df_main = df_jobs_main[df_jobs_main['avg_month_pay'] > 0]
    avg_salary_overall_main = avg_salary_overall_df_main['avg_month_pay'].mean() if not avg_salary_overall_df_main.empty else 0
    main_col4_metrics.metric("ğŸ’° å†å²å¹³å‡æœˆè–ª (K)", f"{avg_salary_overall_main:,.1f}" if avg_salary_overall_main > 0 else "N/A")
    st.markdown("---")

    st.subheader("ğŸ“… å†å²æ•°æ® - è¶‹åŠ¿ä¸åˆ†å¸ƒ") # Changed subheader
    
    # çƒ­é—¨è¶‹åŠ¿å¯è§†åŒ–ä¼˜åŒ–
    trend_col1, trend_col2 = st.columns(2)
    with trend_col1:
        st.markdown("###### ğŸ“ˆ æ¯æ—¥å²—ä½å‘å¸ƒæ•°é‡è¶‹åŠ¿")
        if 'publish_date_dt' in df_jobs_main.columns:
            # ä½¿ç”¨ 'D' è¡¨ç¤ºæ¯æ—¥é¢‘ç‡
            daily_jobs_series_main = get_time_series_data(df_jobs_main, time_col='publish_date_dt', freq='D')
            if not daily_jobs_series_main.empty:
                # åŠ¨æ€å†³å®šå›æº¯æœŸï¼Œæ¯”å¦‚æœ€å¤šæ˜¾ç¤ºæœ€è¿‘180å¤©çš„æ•°æ®ï¼Œå¦‚æœæ•°æ®å°‘äºæ­¤åˆ™å…¨éƒ¨æ˜¾ç¤º
                lookback_days_main = 180 if len(daily_jobs_series_main) > 180 else None 
                plot_line_chart(daily_jobs_series_main, "å†å²æ•°æ® - æ¯æ—¥å²—ä½å‘å¸ƒæ•°é‡", y_label="å²—ä½æ•°é‡", default_lookback_days=lookback_days_main)
            else: st.caption("æ—¥å²—ä½å‘å¸ƒè¶‹åŠ¿æ•°æ®ä¸è¶³ã€‚")
        else: st.caption("ç¼ºå°‘å‘å¸ƒæ—¥æœŸä¿¡æ¯ï¼Œæ— æ³•å±•ç¤ºè¶‹åŠ¿ã€‚")
        
        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("###### ğŸ’¼ çƒ­é—¨æ‹›è˜çœä»½ (Top 10)")
        # æ­£ç¡®ç»Ÿè®¡çœä»½çš„å²—ä½æ•°
        top_provinces_main = get_top_n_counts(df_jobs_main, 'province_clean', top_n=10)
        if not top_provinces_main.empty:
            plot_bar_chart(top_provinces_main, 'province_clean', 'count', "å†å²æ•°æ® - çƒ­é—¨æ‹›è˜çœä»½", "çœä»½", "å²—ä½æ•°é‡", orientation='h') # Horizontal for better readability
        else: st.caption("çœä»½æ•°æ®ä¸è¶³ã€‚")

    with trend_col2:
        st.markdown("###### ğŸ“Š æ•´ä½“å¹³å‡æœˆè–ªåˆ†å¸ƒ (K/æœˆ)")
        if not avg_salary_overall_df_main.empty:
            # ä¸ºè–ªèµ„åˆ†å¸ƒåˆ›å»ºæ›´ç»†è‡´çš„åˆ†ç®±
            salary_bins = [0, 5, 8, 10, 12, 15, 18, 20, 25, 30, 40, 50, 70, 100, 500] # Max 500k for very high salaries
            salary_labels = [f"{salary_bins[i]}-{salary_bins[i+1]}K" for i in range(len(salary_bins)-1)]
            
            df_jobs_main_for_hist = avg_salary_overall_df_main.copy()
            df_jobs_main_for_hist['salary_group_main'] = pd.cut(df_jobs_main_for_hist['avg_month_pay'], bins=salary_bins, labels=salary_labels, right=False)
            
            salary_group_counts_main = df_jobs_main_for_hist['salary_group_main'].value_counts().reset_index()
            salary_group_counts_main.columns = ['salary_group_main', 'count']
            salary_group_counts_main['salary_group_main'] = pd.Categorical(salary_group_counts_main['salary_group_main'], categories=salary_labels, ordered=True)
            salary_group_counts_main.sort_values('salary_group_main', inplace=True)

            fig_hist_main = px.bar(salary_group_counts_main, x="salary_group_main", y="count", 
                                   title="å†å²æ•°æ® - æ•´ä½“å¹³å‡æœˆè–ªåˆ†å¸ƒ", 
                                   labels={'salary_group_main': 'å¹³å‡æœˆè–ªèŒƒå›´ (K)', 'count': 'å²—ä½æ•°é‡'},
                                   text_auto=True) # Show counts on bars
            fig_hist_main.update_layout(bargap=0.2, xaxis_tickangle=-45)
            st.plotly_chart(fig_hist_main, use_container_width=True)
        else: st.caption("è–ªèµ„æ•°æ®ä¸è¶³ä»¥ç”Ÿæˆåˆ†å¸ƒå›¾ã€‚")

        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("###### ğŸ“š çƒ­é—¨èŒä½ç±»åˆ« (Top 7)")
        top_categories_main = get_top_n_counts(df_jobs_main, 'job_catory', top_n=7)
        if not top_categories_main.empty:
            plot_pie_chart(top_categories_main, 'job_catory', 'count', "å†å²æ•°æ® - çƒ­é—¨èŒä½ç±»åˆ«", hole=0.4) # Increased hole
        else: st.caption("èŒä½ç±»åˆ«æ•°æ®ä¸è¶³ã€‚")
        
    st.caption("æ›´è¯¦ç»†çš„å†å²æ•°æ®åˆ†æè¯·æŸ¥çœ‹åº”ç”¨å†…çš„å…¶ä»–åˆ†æé¡µé¢ã€‚")
else:
    st.info("â„¹ï¸ ä¸»æ•°æ®åº“ä¸­æš‚æ— å†å²æ•°æ®å¯ä¾›æ¦‚è§ˆã€‚")

st.divider()

# --- è¿‘æœŸçƒ­é—¨/é«˜è–ªå²—ä½ä¸€è§ˆ ---
st.header("âœ¨ è¿‘æœŸä¸é«˜è–ªå²—ä½èšç„¦ (åŸºäºå†å²æ•°æ®)") # Adjusted header

if not df_jobs_main.empty and 'publish_date_dt' in df_jobs_main.columns:
    col_latest, col_high_salary = st.columns(2)

    with col_latest:
        st.subheader("ğŸ†• æœ€æ–°å‘å¸ƒçš„å²—ä½ (Top 5)")
        latest_jobs_sample = df_jobs_main.sort_values(by='publish_date_dt', ascending=False).head(5)
        if not latest_jobs_sample.empty:
            for idx, row in latest_jobs_sample.iterrows():
                job_title = row.get('job_name', 'N/A')
                company = row.get('company_name', 'N/A')
                # ä½¿ç”¨ province_clean å’Œ city_clean
                province = row.get('province_clean', '')
                city = row.get('city_clean', 'N/A')
                location_str = f"{province} - {city}" if province and province != city else city

                salary_avg = row.get('avg_month_pay', 0)
                salary_str = f"{salary_avg:.1f}K" if salary_avg > 0 else "é¢è®®"
                
                publish_dt = row.get('publish_date_dt')
                date_str = publish_dt.strftime('%Y-%m-%d') if pd.notna(publish_dt) else "æœªçŸ¥æ—¥æœŸ"
                
                st.markdown(f"""
                <div style="border-left: 5px solid #007bff; padding: 10px; margin-bottom: 10px; background-color: #f8f9fa; border-radius: 3px;">
                    <strong>{job_title}</strong> @ <span style="color: #17a2b8;">{company}</span><br>
                    <small>ğŸ“ {location_str} | ğŸ’° {salary_str} | ğŸ—“ï¸ {date_str}</small>
                </div>
                """, unsafe_allow_html=True)
        else:
            st.caption("æš‚æ— æœ€æ–°å²—ä½ä¿¡æ¯ã€‚")

    with col_high_salary:
        st.subheader("ğŸ† è¿‘æœŸé«˜è–ªå²—ä½ (Top 5)")
        # å®šä¹‰â€œè¿‘æœŸâ€ï¼šæ¯”å¦‚æœ€è¿‘30å¤©å†…å‘å¸ƒçš„
        from pandas.tseries.offsets import MonthEnd # For date calculations
        if 'publish_date_dt' in df_jobs_main.columns:
            # ç¡®ä¿ 'publish_date_dt' æ˜¯ datetime ç±»å‹å¹¶ä¸”æœ‰æ—¶åŒºä¿¡æ¯ï¼ˆæˆ–ç»Ÿä¸€å¤„ç†ï¼‰
            # preprocess_jobs_data åº”è¯¥å·²ç»å¤„ç†äº†æ—¶åŒºä¸º UTC
            thirty_days_ago = pd.Timestamp.now(tz='UTC') - pd.Timedelta(days=30) # Use UTC now
            
            recent_high_salary_jobs = df_jobs_main[
                (df_jobs_main['publish_date_dt'] >= thirty_days_ago) & 
                (df_jobs_main['avg_month_pay'] > 0) # ç¡®ä¿æœ‰è–ªèµ„æ•°æ®
            ].sort_values(by='avg_month_pay', ascending=False).head(5)

            if not recent_high_salary_jobs.empty:
                for idx, row in recent_high_salary_jobs.iterrows():
                    job_title = row.get('job_name', 'N/A')
                    company = row.get('company_name', 'N/A')
                    province = row.get('province_clean', '')
                    city = row.get('city_clean', 'N/A')
                    location_str = f"{province} - {city}" if province and province != city else city

                    salary_avg = row.get('avg_month_pay', 0)
                    salary_str = f"{salary_avg:.1f}K" # é«˜è–ªå²—ä½å¿…ç„¶>0
                    
                    publish_dt = row.get('publish_date_dt')
                    date_str = publish_dt.strftime('%Y-%m-%d') if pd.notna(publish_dt) else "æœªçŸ¥æ—¥æœŸ"

                    st.markdown(f"""
                    <div style="border-left: 5px solid #28a745; padding: 10px; margin-bottom: 10px; background-color: #f8f9fa; border-radius: 3px;">
                        <strong>{job_title}</strong> @ <span style="color: #17a2b8;">{company}</span><br>
                        <small>ğŸ“ {location_str} | ğŸ’° <strong style="color: #dc3545;">{salary_str}</strong> | ğŸ—“ï¸ {date_str}</small>
                    </div>
                    """, unsafe_allow_html=True)
            else:
                st.caption("è¿‘30å¤©å†…æš‚æ— ç¬¦åˆæ¡ä»¶çš„é«˜è–ªå²—ä½ã€‚")
        else:
            st.caption("ç¼ºå°‘å‘å¸ƒæ—¥æœŸï¼Œæ— æ³•ç­›é€‰è¿‘æœŸé«˜è–ªå²—ä½ã€‚")
elif not df_jobs_main.empty:
    st.info("å†å²æ•°æ®ä¸­ç¼ºå°‘å‘å¸ƒæ—¥æœŸ (`publish_date_dt`)ï¼Œæ— æ³•å±•ç¤ºè¿‘æœŸå²—ä½ã€‚")
else: # df_jobs_main is empty
    pass # é”™è¯¯å·²åœ¨ä¸»æ•°æ®æ¦‚è§ˆéƒ¨åˆ†å¤„ç†

st.markdown("---")
st.info("ğŸ’¡ å°æç¤º: æœ¬é¡µçš„å®æ—¶çˆ¬å–åŠŸèƒ½ä¼šæ ¹æ®æ‚¨çš„è¾“å…¥å³æ—¶è·å–æœ€æ–°æ•°æ®è¿›è¡Œå±•ç¤ºã€‚")